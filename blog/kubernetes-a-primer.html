<!DOCTYPE html><html lang="en"><head><link rel="apple-touch-icon" sizes="76x76" href="/static/favicons/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/static/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/static/favicons/favicon-16x16.png"/><link rel="manifest" href="/static/favicons/site.webmanifest"/><link rel="mask-icon" href="/static/favicons/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#000000"/><meta name="google-site-verification" content="vKSVHMcjB0UHqDlUClBt58EHAMSlGsOLJTtR-Eoj_Wk"/><meta name="theme-color" content="#000000"/><link rel="alternate" type="application/rss+xml" href="/index.xml"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="stylesheet" data-href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;500;600;700&amp;display=swap"/><link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous"/><link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous"/><link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous"/><link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous"/><script data-goatcounter="https://shree.goatcounter.com/count" async="" src="//gc.zgo.at/count.js"></script><script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="abhishekshree" data-description="Support me on Buy me a coffee!" data-message="" data-color="#5F7FFF" data-position="Right" data-x_margin="18" data-y_margin="18"></script><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /><meta charSet="utf-8"/><script>!function(){try {var d=document.documentElement.classList;d.remove('light','dark');var e=localStorage.getItem('theme');if(!e)return localStorage.setItem('theme','light'),d.add('light');if("system"===e){var t="(prefers-color-scheme: dark)",m=window.matchMedia(t);m.media!==t||m.matches?d.add('dark'):d.add('light')}else d.add(e)}catch(e){}}()</script><meta content="width=device-width, initial-scale=1" name="viewport"/><meta name="robots" content="index,follow"/><meta name="googlebot" content="index,follow"/><title>Kubernetes, EKS and some Karpenter(y)</title><meta name="robots" content="follow, index"/><meta name="description" content="From container chaos to cluster harmony - understanding K8s, EKS flavors, and why Karpenter even exists."/><meta property="og:url" content="https://abhishekshree.github.io//blog/kubernetes-a-primer"/><meta property="og:type" content="article"/><meta property="og:site_name" content="Abhishek Shree"/><meta property="og:description" content="From container chaos to cluster harmony - understanding K8s, EKS flavors, and why Karpenter even exists."/><meta property="og:title" content="Kubernetes, EKS and some Karpenter(y)"/><meta property="og:image" content="https://abhishekshree.github.io//static/images/twitter-card.png"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="https://twitter.com/AbhishekShree10"/><meta name="twitter:title" content="Kubernetes, EKS and some Karpenter(y)"/><meta name="twitter:description" content="From container chaos to cluster harmony - understanding K8s, EKS flavors, and why Karpenter even exists."/><meta name="twitter:image" content="https://abhishekshree.github.io//static/images/twitter-card.png"/><link rel="canonical" href="https://abhishekshree.github.io//blog/kubernetes-a-primer"/><meta property="article:published_time" content="2025-06-29T00:00:00.000Z"/><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://abhishekshree.github.io//blog/kubernetes-a-primer"
  },
  "headline": "Kubernetes, EKS and some Karpenter(y)",
  "image": [
    {
      "@type": "ImageObject",
      "url": "https://abhishekshree.github.io//static/images/twitter-card.png"
    }
  ],
  "datePublished": "2025-06-29T00:00:00.000Z",
  "dateModified": "2025-06-29T00:00:00.000Z",
  "author": {
    "@type": "Person",
    "name": "Abhishek Shree"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Abhishek Shree",
    "logo": {
      "@type": "ImageObject",
      "url": "https://abhishekshree.github.io/undefined"
    }
  },
  "description": "From container chaos to cluster harmony - understanding K8s, EKS flavors, and why Karpenter even exists."
}</script><meta name="next-head-count" content="22"/><link rel="preload" href="/_next/static/css/ef01ea02e64a14c0132c.css" as="style"/><link rel="stylesheet" href="/_next/static/css/ef01ea02e64a14c0132c.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-a40ef1678bae11e696dba45124eadd70.js"></script><script src="/_next/static/chunks/webpack-fb76148cfcfb42ca18eb.js" defer=""></script><script src="/_next/static/chunks/main-3fad22d46a3c1ce002ac.js" defer=""></script><script src="/_next/static/chunks/pages/_app-af46ad50bb75c7d5b990.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-1559f9e76cb360fe89ca.js" defer=""></script><script src="/_next/static/I-WTZFY5ZKh476NTDoWsD/_buildManifest.js" defer=""></script><script src="/_next/static/I-WTZFY5ZKh476NTDoWsD/_ssgManifest.js" defer=""></script><style data-href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;500;600;700&display=swap">@font-face{font-family:'Inconsolata';font-style:normal;font-weight:400;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/inconsolata/v36/QldgNThLqRwH-OJ1UHjlKENVzkWGVkL3GZQmAwLYxYWI2qfdm7Lpp4U8aRk.woff) format('woff')}@font-face{font-family:'Inconsolata';font-style:normal;font-weight:500;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/inconsolata/v36/QldgNThLqRwH-OJ1UHjlKENVzkWGVkL3GZQmAwLYxYWI2qfdm7Lpp7c8aRk.woff) format('woff')}@font-face{font-family:'Inconsolata';font-style:normal;font-weight:600;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/inconsolata/v36/QldgNThLqRwH-OJ1UHjlKENVzkWGVkL3GZQmAwLYxYWI2qfdm7Lpp1s7aRk.woff) format('woff')}@font-face{font-family:'Inconsolata';font-style:normal;font-weight:700;font-stretch:normal;font-display:swap;src:url(https://fonts.gstatic.com/s/inconsolata/v36/QldgNThLqRwH-OJ1UHjlKENVzkWGVkL3GZQmAwLYxYWI2qfdm7Lpp2I7aRk.woff) format('woff')}@font-face{font-family:'Inconsolata';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/inconsolata/v36/QlddNThLqRwH-OJ1UHjlKENVzkWGVkL3GZQmAwLyxq15IDhunJ_o.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Inconsolata';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/inconsolata/v36/QlddNThLqRwH-OJ1UHjlKENVzkWGVkL3GZQmAwLyx615IDhunJ_o.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Inconsolata';font-style:normal;font-weight:400;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/inconsolata/v36/QlddNThLqRwH-OJ1UHjlKENVzkWGVkL3GZQmAwLyya15IDhunA.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Inconsolata';font-style:normal;font-weight:500;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/inconsolata/v36/QlddNThLqRwH-OJ1UHjlKENVzkWGVkL3GZQmAwLyxq15IDhunJ_o.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Inconsolata';font-style:normal;font-weight:500;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/inconsolata/v36/QlddNThLqRwH-OJ1UHjlKENVzkWGVkL3GZQmAwLyx615IDhunJ_o.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Inconsolata';font-style:normal;font-weight:500;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/inconsolata/v36/QlddNThLqRwH-OJ1UHjlKENVzkWGVkL3GZQmAwLyya15IDhunA.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Inconsolata';font-style:normal;font-weight:600;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/inconsolata/v36/QlddNThLqRwH-OJ1UHjlKENVzkWGVkL3GZQmAwLyxq15IDhunJ_o.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Inconsolata';font-style:normal;font-weight:600;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/inconsolata/v36/QlddNThLqRwH-OJ1UHjlKENVzkWGVkL3GZQmAwLyx615IDhunJ_o.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Inconsolata';font-style:normal;font-weight:600;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/inconsolata/v36/QlddNThLqRwH-OJ1UHjlKENVzkWGVkL3GZQmAwLyya15IDhunA.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Inconsolata';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/inconsolata/v36/QlddNThLqRwH-OJ1UHjlKENVzkWGVkL3GZQmAwLyxq15IDhunJ_o.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Inconsolata';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/inconsolata/v36/QlddNThLqRwH-OJ1UHjlKENVzkWGVkL3GZQmAwLyx615IDhunJ_o.woff2) format('woff2');unicode-range:U+0100-02BA,U+02BD-02C5,U+02C7-02CC,U+02CE-02D7,U+02DD-02FF,U+0304,U+0308,U+0329,U+1D00-1DBF,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Inconsolata';font-style:normal;font-weight:700;font-stretch:100%;font-display:swap;src:url(https://fonts.gstatic.com/s/inconsolata/v36/QlddNThLqRwH-OJ1UHjlKENVzkWGVkL3GZQmAwLyya15IDhunA.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><body class="antialiased text-black bg-white dark:bg-gray-900 dark:text-white"><div id="__next"><div class="max-w-3xl px-4 mx-auto sm:px-6 xl:max-w-5xl xl:px-0"><div class="flex flex-col justify-between h-screen"><header class="flex items-center justify-between py-10"><div><a aria-label="Abhishek&#x27;s Blog" href="/"><div class="flex items-center justify-between mono"><div class="mr-3"><svg xmlns="http://www.w3.org/2000/svg" width="53pt" height="44pt" viewBox="0 0 53 44"><path d="M42.125 24.848h2.887v2.847h-2.887zm0 0" fill="#f47f31"></path><path d="M44.508 21.82v-6.937c0-.79-.77-1.426-1.715-1.426H6.777c-1.925 0-3.48 1.32-3.425 2.93v5.433zm0 0" fill="#eb6a2f"></path><path d="M24.238 19.41H6.778V2.02c0-.758.742-1.375 1.656-1.375h1.773l1.715.3 1.715-.3h8.633c.918 0 1.656.617 1.656 1.375v2.183h.312zm0 0" fill="#7da8ff"></path><path d="M22.27.645h-2.38c.915 0 1.657.617 1.657 1.375v17.39h2.379V2.02c0-.758-.738-1.375-1.656-1.375zm0 0" fill="#7092ff"></path><path d="M10.207.645h3.43V19.41h-3.43zm0 0" fill="#365e7d"></path><path d="M19.668 7.762h-1.773c-.457 0-.829-.305-.829-.688V4.18c0-.38.372-.688.829-.688h1.773c.46 0 .828.309.828.688v2.894c0 .383-.367.688-.828.688zm0 0" fill="#ffdc67"></path><path d="M41.078 19.41V4.203c0-.39-.387-.71-.86-.71H24.786c-.473 0-.855.32-.855.71V19.41h3.43l5.144-1.68 5.144 1.68zm0 0" fill="#95d6a4"></path><path d="M37.648 19.41V8.168c0-.168-.14-.309-.336-.348-.68-.125-1.296-.625-1.449-1.207-.043-.16-.218-.273-.418-.273H29.56c-.2 0-.372.113-.414.277-.153.567-.758 1.078-1.457 1.203-.192.035-.333.184-.333.348V19.41" fill="#ccf49f"></path><path d="M32.898 13.508c-.918-.184-1.843.293-2.062 1.058-.215.762.355 1.532 1.277 1.711.918.18 1.844-.293 2.063-1.058.215-.766-.356-1.531-1.278-1.711zm0 0" fill="#95d6a4"></path><path d="M40.219 3.492h-2.38c.474 0 .856.32.856.711V19.41h2.383V4.203c0-.39-.387-.71-.86-.71zm0 0" fill="#78c2a4"></path><path d="M44.508 41.934c0 .785-.77 1.421-1.715 1.421H6.777c-1.894 0-3.43-1.273-3.43-2.847V16.305l.005.082c.05 1.55 1.628 2.765 3.5 2.765h35.941c.945 0 1.715.637 1.715 1.426zm0 0" fill="#fc974d"></path><path d="M42.793 19.152H40.41c.95 0 1.715.637 1.715 1.426v21.356c0 .785-.766 1.421-1.715 1.421h2.383c.945 0 1.715-.636 1.715-1.421V20.578c0-.789-.77-1.426-1.715-1.426zm0 0" fill="#f47f31"></path><path d="M14.523 40.508H9.32c-.457 0-.828-.309-.828-.688v-1.472c0-.38.371-.688.828-.688h5.203c.457 0 .829.309.829.688v1.472c0 .38-.372.688-.829.688zm0 0" fill="#ffdc67"></path><path d="M44.508 24.848h1.715c1.894 0 3.43 1.273 3.43 2.847v5.696c0 1.57-1.536 2.847-3.43 2.847H34.219c-.95 0-1.715-.64-1.715-1.425v-5.696c0-.785.766-1.422 1.715-1.422h10.289zm0 0" fill="#f47f31"></path><path d="M46.223 24.848h-1.715v.054c1.574.258 2.762 1.41 2.762 2.793v5.696c0 1.57-1.536 2.847-3.43 2.847h2.383c1.894 0 3.43-1.277 3.43-2.847v-5.696c0-1.574-1.536-2.847-3.43-2.847zm0 0" fill="#eb6a2f"></path><path d="M38.203 30.621c-.89-.266-1.871.117-2.195.856-.32.738.14 1.554 1.031 1.824.89.265 1.871-.117 2.195-.856.32-.738-.14-1.554-1.03-1.824zm0 0" fill="#ffdc67"></path><path d="M16.129 39.82v-1.472c0-.735-.719-1.332-1.606-1.332H9.32c-.883 0-1.601.597-1.601 1.332v1.472c0 .735.718 1.332 1.601 1.332h5.203c.887 0 1.606-.597 1.606-1.332zm-6.86 0v-1.472c0-.024.024-.043.051-.043h5.203c.032 0 .055.02.055.043v1.472c0 .024-.023.043-.055.043H9.32c-.027 0-.05-.02-.05-.043zm0 0M37.648 29.898c-1.375 0-2.492.926-2.492 2.067 0 1.14 1.117 2.07 2.492 2.07 1.372 0 2.489-.93 2.489-2.07 0-1.14-1.117-2.067-2.489-2.067zm0 2.848c-.52 0-.941-.351-.941-.781 0-.43.422-.777.941-.777.516 0 .938.347.938.777 0 .43-.422.781-.938.781zm0 0"></path><path d="M46.223 24.203h-.942v-9.32c0-1.14-1.117-2.07-2.488-2.07h-.941v-8.61c0-.746-.73-1.355-1.633-1.355H24.785c-.027 0-.055.004-.082.004V2.02c0-1.114-1.09-2.02-2.43-2.02H8.433c-1.34 0-2.43.906-2.43 2.02v2.32c0 .355.349.644.774.644.43 0 .778-.289.778-.644V2.02c0-.403.394-.73.879-.73h1v17.218h-1.88V6.918c0-.356-.347-.645-.777-.645-.425 0-.773.29-.773.645v5.953c-.848.133-1.625.473-2.242 1-.762.652-1.184 1.504-1.188 2.414v24.223C2.574 42.434 4.461 44 6.777 44h3.594c.43 0 .777-.29.777-.645 0-.355-.347-.644-.777-.644H6.777c-1.46 0-2.652-.988-2.652-2.203V18.984c.71.485 1.61.786 2.582.809.023.004 36.086.004 36.086.004.516 0 .937.351.937.781v6.473H34.22c-1.375 0-2.492.926-2.492 2.066v5.695c0 1.141 1.117 2.07 2.492 2.07h9.511v5.052c0 .43-.421.777-.937.777H13.477c-.43 0-.778.289-.778.644 0 .356.348.645.778.645h29.316c1.371 0 2.488-.93 2.488-2.066v-5.051h.942c2.316 0 4.203-1.567 4.203-3.492v-5.696c0-1.925-1.887-3.492-4.203-3.492zM4.125 16.367c-.02-.601.246-1.168.75-1.597.32-.27.703-.465 1.129-.57v4.195c-1.063-.286-1.848-1.086-1.879-2.028zm20.66-12.23H40.22c.047 0 .082.031.082.066v14.305h-1.88V8.168c0-.473-.386-.875-.94-.977-.395-.074-.774-.386-.86-.714-.121-.454-.617-.782-1.176-.782H29.56c-.567 0-1.051.32-1.172.782-.09.328-.469.644-.864.714-.546.102-.941.512-.941.977v10.34h-1.879V4.203c0-.035.04-.066.082-.066zm12.086 14.37h-8.738V8.388c.773-.223 1.422-.762 1.691-1.403h5.36c.265.641.914 1.18 1.687 1.403zM23.152 2.02v16.488h-8.738V1.289h7.856c.488 0 .882.328.882.73zM10.984 18.508V1.289h1.88v17.219zm31.809 0h-.941v-4.406h.941c.516 0 .937.351.937.78v3.778a2.93 2.93 0 00-.937-.152zm6.082 14.883c0 1.214-1.191 2.203-2.652 2.203H34.219c-.52 0-.942-.352-.942-.782v-5.695c0-.43.422-.777.942-.777h11.144c.43 0 .778-.29.778-.645 0-.355-.348-.644-.778-.644h-.082v-1.559h.942c1.46 0 2.652.988 2.652 2.203zm0 0"></path><path d="M32.504 12.813c-1.375 0-2.492.93-2.492 2.07 0 1.14 1.117 2.066 2.492 2.066 1.371 0 2.488-.926 2.488-2.066 0-1.14-1.117-2.07-2.488-2.07zm0 2.847c-.52 0-.941-.348-.941-.777 0-.43.421-.781.94-.781.517 0 .938.351.938.78 0 .43-.421.778-.937.778zm0 0M32.504 11.254c.426 0 .773-.29.773-.645V9.188c0-.356-.347-.645-.773-.645-.43 0-.777.289-.777.645v1.421c0 .356.347.645.777.645zm0 0M17.895 8.406h1.773c.887 0 1.605-.597 1.605-1.332V4.18c0-.735-.718-1.332-1.605-1.332h-1.773c-.883 0-1.602.597-1.602 1.332v2.894c0 .735.719 1.332 1.602 1.332zm-.051-4.226c0-.024.023-.043.05-.043h1.774c.031 0 .055.02.055.043v2.894c0 .024-.024.043-.055.043h-1.773c-.028 0-.051-.02-.051-.043zm0 0"></path></svg></div><div class="hidden h-6 text-2xl font-semibold sm:block">shree_e</div></div></a></div><div class="flex items-center text-base leading-5"><div class="hidden sm:block"><a class="m-1 text-gray-900 sm:m-4 dark:text-gray-100" href="/"><span class="strikethrough dark:strikethrough-dark">Home</span></a><a class="m-1 text-gray-900 sm:m-4 dark:text-gray-100" href="/blog"><span class="strikethrough dark:strikethrough-dark">Blog</span></a><a class="m-1 text-gray-900 sm:m-4 dark:text-gray-100" href="/work"><span class="strikethrough dark:strikethrough-dark">Work</span></a><a class="m-1 text-gray-900 sm:m-4 dark:text-gray-100" href="/projects"><span class="strikethrough dark:strikethrough-dark">Projects</span></a><a class="m-1 text-gray-900 sm:m-4 dark:text-gray-100" href="/contact"><span class="strikethrough dark:strikethrough-dark">Contact</span></a><a class="m-1 text-gray-900 sm:m-4 dark:text-gray-100" href="/static/cv/cv.pdf"><span class="strikethrough dark:strikethrough-dark">CV</span></a></div><button aria-label="Toggle Dark Mode" type="button" class="ml-1 mr-1 h-8 w-8 rounded p-1 sm:ml-4"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="text-gray-900 dark:text-gray-100"><path d="M17.293 13.293A8 8 0 016.707 2.707a8.001 8.001 0 1010.586 10.586z"></path></svg></button><div class="sm:hidden"><button type="button" class="w-8 h-8 ml-1 mr-1 rounded" aria-label="Toggle Menu"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="text-gray-900 dark:text-gray-100"><path fill-rule="evenodd" d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 10a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 15a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1z" clip-rule="evenodd"></path></svg></button><div class="fixed w-full h-full top-24 right-0 bg-gray-200 dark:bg-gray-800 opacity-95 z-10 transform ease-in-out duration-300 translate-x-full"><button type="button" aria-label="toggle modal" class="fixed w-full h-full cursor-auto focus:outline-none"></button><nav class="fixed h-full mt-8"><div class="px-12 py-4"><a class="text-2xl font-bold tracking-widest text-gray-900 dark:text-gray-100" href="/">Home</a></div><div class="px-12 py-4"><a class="text-2xl font-bold tracking-widest text-gray-900 dark:text-gray-100" href="/blog">Blog</a></div><div class="px-12 py-4"><a class="text-2xl font-bold tracking-widest text-gray-900 dark:text-gray-100" href="/work">Work</a></div><div class="px-12 py-4"><a class="text-2xl font-bold tracking-widest text-gray-900 dark:text-gray-100" href="/projects">Projects</a></div><div class="px-12 py-4"><a class="text-2xl font-bold tracking-widest text-gray-900 dark:text-gray-100" href="/contact">Contact</a></div><div class="px-12 py-4"><a class="text-2xl font-bold tracking-widest text-gray-900 dark:text-gray-100" href="/static/cv/cv.pdf">CV</a></div></nav></div></div></div></header><main class="mb-auto"><div class="max-w-3xl px-4 mx-auto sm:px-6 xl:max-w-5xl xl:px-0"><div class="fixed right-8 bottom-8 hidden flex-col gap-3 md:hidden"><button aria-label="Scroll To Top" type="button" class="rounded-full bg-gray-200 p-2 text-gray-500 transition-all hover:bg-gray-300 dark:bg-gray-700 dark:text-gray-400 dark:hover:bg-gray-600"><svg class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M3.293 9.707a1 1 0 010-1.414l6-6a1 1 0 011.414 0l6 6a1 1 0 01-1.414 1.414L11 5.414V17a1 1 0 11-2 0V5.414L4.707 9.707a1 1 0 01-1.414 0z" clip-rule="evenodd"></path></svg></button></div><article><div class="xl:divide-y xl:divide-gray-200 xl:dark:divide-gray-700"><header class="pt-6 xl:pb-6"><div class="space-y-1 text-center"><dl class="space-y-10"><div><dt class="sr-only">Published on</dt><dd class="text-base mono font-medium leading-6 text-gray-500 dark:text-gray-400"><time dateTime="2025-06-29">Sunday, June 29, 2025</time></dd></div></dl><div><h1 class="text-3xl font-extrabold leading-9 tracking-tight text-gray-900 dark:text-gray-100 sm:text-4xl sm:leading-10 md:text-5xl md:leading-14">Kubernetes, EKS and some Karpenter(y)</h1></div></div></header><div class="pb-8 divide-y divide-gray-200 xl:divide-y-0 dark:divide-gray-700 xl:grid xl:grid-cols-4 xl:gap-x-6" style="grid-template-rows:auto 1fr"><dl class="pt-6 pb-10 xl:pt-11 xl:border-b xl:border-gray-200 xl:dark:border-gray-700"><dt class="sr-only">Authors</dt><dd><ul class="flex justify-center space-x-8 xl:block sm:space-x-12 xl:space-x-0 xl:space-y-8"><li class="flex items-center space-x-2"><img src="/static/images/dp.jpg" alt="avatar" class="w-10 h-10 rounded-full"/><dl class="text-sm font-medium leading-5 whitespace-nowrap"><dt class="sr-only">Name</dt><dd class="text-gray-900 dark:text-gray-100">Abhishek Shree</dd><dt class="sr-only">Facebook</dt><dd><a target="_blank" rel="noopener noreferrer" style="text-decoration:none" href="https://facebook.com/abhishek.shree30" class="text-blue-500 hover:text-blue-600 dark:hover:text-blue-400">@Abhishek</a></dd></dl></li></ul></dd></dl><div class="divide-y divide-gray-200 dark:divide-gray-700 xl:pb-0 xl:col-span-3 xl:row-span-2"><div class="pt-10 pb-8 prose dark:prose-dark max-w-none"><div><p>Picture this: It&#x27;s 2021, and I&#x27;m watching a friend at a B2B SaaS startup manually deploying updates to their customer data platform. SSH into server 1, pull the latest code, restart the service. SSH into server 2, repeat. Pray nothing breaks. Fast forward to some time later, that same company is processing millions of events daily across hundreds of microservices, all orchestrated by Kubernetes.</p><h1 id="first-encounter-with-kubernetes"><a href="#first-encounter-with-kubernetes" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>First encounter with Kubernetes</h1><p>Modern B2B SaaS companies like whatever, Slack, Paradime ;), etc are distributed computing platforms represented as simple web apps. Behind Slack&#x27;s chat interface are hundreds of microservices handling message routing, file uploads, search indexing, and notifications. Saw a similar thing with Paradime. Each service needs independent deployment, scaling, and updates across multiple regions, customers, etc. It&#x27;s pretty cool to see how you keep many tenants under a centrally managable cluster and cater them with updates, patches, and new features without downtime, together.</p><h1 id="understanding-kubernetes-orchestration"><a href="#understanding-kubernetes-orchestration" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>Understanding Kubernetes Orchestration</h1><p>Let&#x27;s clear up the infrastructure confusion first. A <strong>cluster</strong> spans multiple physical machines (nodes). Each <strong>pod</strong> runs on a single node - you can&#x27;t split pods across machines, but you can run multiple pods of the same app on different nodes.</p><h2 id="how-k8s-orchestration-actually-works"><a href="#how-k8s-orchestration-actually-works" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>How K8s Orchestration Actually Works</h2><p>Kubernetes orchestration is essentially a distributed state machine. The <strong>API server</strong> acts as the central state store, etcd holds the persistent state, and various controllers continuously work to make reality match your desired configuration.</p><p>A simple orchestration flow when you deploy an application could be,</p><pre><code>// Kubernetes Orchestration Flow
kubectl apply → API Server → etcd (stores desired state)
                    ↓
            Controller Manager watches changes
                    ↓
    ReplicaSet Controller creates Pod objects
                    ↓
        Scheduler assigns Pods to Nodes
                    ↓
    Kubelet on Node pulls images and starts containers
                    ↓
        kube-proxy configures networking rules
</code></pre><p>It is so much more than just container orchestration, managing networking, persistent storage, and resource allocation. Pretty cool stuff.</p><p>The <strong>Controller Manager</strong> runs multiple control loops, each responsible for a specific resource type. The ReplicaSet controller ensures you have the right number of pod replicas. The Deployment controller manages ReplicaSets during rolling updates. The Service controller creates endpoints for load balancing. There are probably a thousands of other blogs you could read about each of the units, so let&#x27;s just leave it there. </p><p>The scheduler&#x27;s algorithm, I find pretty cool, is more sophisticated than just resource availability:</p><pre><code>// Kubernetes Scheduler (simplified)
function schedulePod(pod) {
    eligibleNodes = filterNodes(pod.requirements)
    
    for each node in eligibleNodes {
        score = 0
        score += leastRequestedPriority(node)  // (capacity - requests) / capacity
        score += nodeAffinityPriority(node, pod)
        score += balancedResourceAllocation(node)
        score += imageLocalityPriority(node, pod) // prefer nodes with images cached
        score += interPodAffinityPriority(node, pod) // pod placement preferences
        node.totalScore = score
    }
    
    return node with highest totalScore
}
</code></pre><p>The networking orchestration is particularly clever. Every pod gets its own IP address within a cluster-wide subnet. The <strong>Container Network Interface (CNI)</strong> plugin handles the actual networking implementation. When a pod starts, the kubelet calls the CNI plugin to:</p><ul><li>Create a network namespace for the pod</li><li>Assign an IP address</li><li>Configure routing rules</li><li>Set up network policies</li></ul><h2 id="how-pod-ips-actually-get-handed-out"><a href="#how-pod-ips-actually-get-handed-out" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>How Pod IPs Actually Get Handed Out</h2><p>Picture Kubernetes as a very organized librarian managing IP addresses. When you create a cluster, you&#x27;re essentially saying &quot;here&#x27;s my IP address book&quot; - typically something like <code>10.244.0.0/16</code>. That&#x27;s your cluster CIDR, and it&#x27;s finite. 65,536 IPs to be exact.</p><p>The distribution looks like this:</p><pre><code>Your Cluster&#x27;s IP Real Estate:
10.244.0.0/16 (65,536 total addresses)
    ↓
Node 1 gets: 10.244.0.0/24    (256 addresses)
Node 2 gets: 10.244.1.0/24    (256 addresses)  
Node 3 gets: 10.244.2.0/24    (256 addresses)
...you get the pattern
</code></pre><p>Each node becomes a mini IP manager. When a pod starts up:</p><ol><li>Kubelet taps the CNI plugin on the shoulder: &quot;Hey, need an IP for this new pod&quot;</li><li>CNI plugin checks its local notebook: &quot;Let&#x27;s see... 10.244.1.47 is free, there you go&quot;</li><li>Creates a network namespace, assigns the IP, updates the books</li><li>Pod gets its own little network identity</li></ol><p>Pretty elegant, right? Until reality crashes the party.</p><h2 id="the-math-that-ruins-your-day"><a href="#the-math-that-ruins-your-day" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>The Math That Ruins Your Day</h2><p>Here&#x27;s where that &quot;infinite pods&quot; dream hits some very finite walls:</p><h3 id="ip-address-space---its-not-actually-infinite"><a href="#ip-address-space---its-not-actually-infinite" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>IP Address Space - It&#x27;s Not Actually Infinite</h3><p>That <code>/16</code> CIDR? You&#x27;re looking at ~65K IPs total. Sounds like a lot until you realize you&#x27;re carving it up into <code>/24</code> chunks (256 IPs per node). You can theoretically have ~256 nodes, but you&#x27;re losing addresses to:</p><ul><li>Network and broadcast addresses (because networking protocols are picky)</li><li>Service IPs (different CIDR but still affects your planning)</li><li>System pods that you forgot about</li><li>That one guy who hardcoded some random IP somewhere (you know who you are)</li></ul><p>Read a bit about EKS as well, AWS has some opinions about your pod density dreams. They have a hard limit on how many pods can run per instance type based on the networking capabilities of the underlying hardware, like:</p><pre><code>m5.large:   29 pods max   (networking hardware says no)
m5.xlarge:  58 pods max   (double the CPU, not double the pods)
m5.2xlarge: 58 pods max   (same networking limits, just prettier)
c5.24xlarge: 737 pods max (now we&#x27;re talking, lfggg)
</code></pre><p>Since we are talking networks, it&#x27;ll be inhuman of me to not take a dig on <a target="_blank" rel="noopener noreferrer" style="text-decoration:none" href="https://dryairship.github.io/post/when-i-messed-up-the-gymkhana-server/">this crisis</a> which could&#x27;ve been avoided and everyone at IITK would still have wifi access if we had such network isolation. </p><p>Services provide stable endpoints for pod groups using kube-proxy, which implements load balancing through iptables rules or IPVS. When you create a service, kube-proxy updates iptables on every node to redirect traffic to healthy pod IPs.</p><h2 id="resource-orchestration-deep-dive"><a href="#resource-orchestration-deep-dive" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>Resource Orchestration Deep Dive</h2><p>Resource sharing uses Linux <strong>cgroups</strong> for enforcement and the <strong>Completely Fair Scheduler (CFS)</strong> for CPU time allocation. When you request 250m CPU, that&#x27;s 250 millicores (0.25 cores). Here&#x27;s how it works:</p><pre><code>// Resource allocation logic
function allocateResources(pod, node) {
    // CPU is compressible - can be throttled
    cpuShares = pod.cpuRequest * 1024 / 1000  // Convert millicores to CPU shares
    setCgroupLimit(&quot;/sys/fs/cgroup/cpu/pod123/cpu.shares&quot;, cpuShares)
    
    // Memory is incompressible - OOM kill if exceeded
    memoryLimit = pod.memoryLimit
    setCgroupLimit(&quot;/sys/fs/cgroup/memory/pod123/memory.limit_in_bytes&quot;, memoryLimit)
}
</code></pre><p>A 4-core node (4000m total) can run 16 containers each requesting 250m, but only if they don&#x27;t all peak simultaneously. The CFS ensures fair time slicing among containers sharing the same core. Very very cool stuff.</p><p>The orchestration extends to storage through <strong>Container Storage Interface (CSI)</strong>. When a pod needs persistent storage, the scheduler considers volume topology, the CSI driver provisions storage, and the kubelet mounts it into the container.</p><h1 id="what-are-fargates"><a href="#what-are-fargates" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>What Are Fargates?</h1><p>Fargate is AWS&#x27;s serverless compute engine that runs containers without managing infrastructure. Think Lambda for containers with more flexibility.</p><p>Traditional containers require provisioning EC2 instances, managing OS, security patches, and monitoring. Fargate abstracts this away. You define container specs (CPU, memory), AWS handles everything else.</p><p>The magic happens with Firecracker microVMs. Each Fargate task runs in its own lightweight VM, providing hardware-level isolation. This is why Fargate has longer cold starts than regular containers but better security boundaries. </p><h1 id="eks---awss-kubernetes-distribution"><a href="#eks---awss-kubernetes-distribution" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>EKS - AWS&#x27;s Kubernetes Distribution</h1><p>AWS looked at Kubernetes control plane complexity and said &quot;what if we made this not terrible?&quot; Setting up production K8s involves configuring etcd, API server, scheduler, controller manager, certificates, and high availability. It&#x27;s a nightmare.</p><p>EKS handles this complexity. You get a managed control plane across multiple AZs with automatic backups, patching, and monitoring. Just an HTTPS endpoint to interact with your cluster.</p><h2 id="eks-with-ec2-nodes"><a href="#eks-with-ec2-nodes" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>EKS with EC2 Nodes</h2><p>The classic approach. You get actual EC2 instances as worker nodes running the kubelet, kube-proxy, and container runtime.</p><p><strong>Resource Sharing</strong>: Traditional Kubernetes resource model. Multiple pods share the same EC2 instance&#x27;s CPU, memory, and network. A single m5.large (2 vCPUs, 8GB RAM) might run 10-20 small pods efficiently. The kubelet enforces resource limits using cgroups, and pods compete for resources using the CFS scheduler.</p><p><strong>Pros</strong>: Full control, SSH access, persistent storage, privileged containers, GPU support
<strong>Cons</strong>: You manage OS, patches, node lifecycle, capacity planning</p><h2 id="eks-with-fargate"><a href="#eks-with-fargate" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>EKS with Fargate</h2><p>Serverless containers where each pod runs in its own Firecracker microVM.</p><p><strong>Resource Sharing</strong>: No sharing! Each pod gets dedicated CPU and memory allocation. You specify exact resource requirements (0.25 vCPU, 0.5GB RAM up to 4 vCPU, 30GB RAM), and AWS provisions that exact capacity. Think of it as &quot;right-sized VMs&quot; for each workload.</p><pre><code>// Fargate resource allocation
Pod A: 0.5 vCPU, 1GB RAM → Gets dedicated Fargate task with those exact resources
Pod B: 1 vCPU, 2GB RAM   → Gets separate dedicated Fargate task
</code></pre><h2 id="mixed-mode-resource-management"><a href="#mixed-mode-resource-management" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>Mixed Mode Resource Management</h2><p>This is where it gets interesting. EKS can intelligently distribute workloads across both compute types based on your specifications:</p><pre><code>// Mixed mode scheduling
function scheduleWorkload(pod) {
    if (pod.hasLabel(&quot;compute-type&quot;, &quot;fargate&quot;)) {
        scheduleOnFargate(pod)  // Dedicated resources
    } else if (pod.requiresGPU() || pod.isPrivileged()) {
        scheduleOnEC2(pod)      // Shared EC2 resources
    } else {
        optimizeForCost(pod)    // Scheduler picks best option
    }
}
</code></pre><p><strong>Resource Optimization Strategies</strong>:</p><ul><li><strong>Steady-state services</strong>: Run on EC2 for cost efficiency through resource sharing</li><li><strong>Batch jobs</strong>: Use Fargate for predictable resource allocation and no capacity planning</li><li><strong>Spiky workloads</strong>: Fargate for auto-scaling without pre-provisioned capacity</li><li><strong>Long-running stateful services</strong>: EC2 with persistent storage</li></ul><p>The EKS scheduler considers both resource availability and cost optimization. A CPU-intensive batch job might run on Fargate for dedicated performance, while a low-resource API service runs on shared EC2 instances for cost efficiency.</p><h1 id="karpenter"><a href="#karpenter" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>Karpenter</h1><p>Traditional Cluster Autoscaler is dumb. It monitors unschedulable pods and scales predefined node groups. Need high-memory workloads but only have m5.large node groups? Tough luck convinving AWS team that.</p><p>Karpenter implements just-in-time provisioning with intelligent instance selection:</p><pre><code>// Karpenter&#x27;s provisioning algorithm (simplified)
function provisionNodes(unschedulablePods) {
    requirements = aggregateRequirements(unschedulablePods)
    
    candidates = []
    for each instanceType in EC2_INSTANCE_TYPES {
        if meetsRequirements(instanceType, requirements) {
            score = calculateScore(instanceType, requirements)
            candidates.add({type: instanceType, score: score})
        }
    }
    
    // Multi-objective optimization
    optimal = candidates.sortBy(cost, performance, availability).first()
    return launchInstance(optimal.type)
}
</code></pre><p>The scoring considers cost, performance, availability, and capacity efficiency. Karpenter also implements consolidation algorithms, continuously monitoring utilization and moving pods to pack them efficiently. </p><p>For spot instances, Karpenter implements diversification strategies, spreading workloads across instance families and AZs. When spot interruption arrives, it gracefully drains nodes and provisions replacement capacity. You can check it <a target="_blank" rel="noopener noreferrer" style="text-decoration:none" href="https://github.com/aws/karpenter-provider-aws/blob/main/designs/consolidation.md">here</a> if you want to read more about the consolidation strategies. I was intrigued, also this blog became massive. </p><hr/><ul><li><a target="_blank" rel="noopener noreferrer" style="text-decoration:none" href="https://www.eksworkshop.com/">EKS Workshop</a></li><li><a target="_blank" rel="noopener noreferrer" style="text-decoration:none" href="https://kubernetes.io/docs/concepts/scheduling-eviction/">Kubernetes Scheduling Deep Dive</a></li><li><a target="_blank" rel="noopener noreferrer" style="text-decoration:none" href="https://www.youtube.com/watch?v=lkg_9ETHeks">AWS re:Invent Karpenter Talks</a></li></ul></div></div><div class="pt-6 pb-6 text-sm text-gray-500 dark:text-gray-300">Hi, In case you want to discuss anything about this post, you can reach out to me over<!-- --> <span class="text-blue-500 hover:text-blue-600 dark:hover:text-blue-400"><a href="/contact">here</a></span>.</div></div><footer><div class="text-sm font-medium leading-5 divide-gray-200 xl:divide-y dark:divide-gray-700 xl:col-start-1 xl:row-start-2"><div class="py-4 xl:py-8"><h2 class="text-xs tracking-wide text-gray-500 uppercase dark:text-gray-400">Tags</h2><div class="flex flex-wrap"><a class="mr-3 text-sm font-medium text-blue-500 uppercase hover:text-blue-600 dark:hover:text-blue-400" href="/tags/kubernetes">kubernetes</a><a class="mr-3 text-sm font-medium text-blue-500 uppercase hover:text-blue-600 dark:hover:text-blue-400" href="/tags/aws">aws</a><a class="mr-3 text-sm font-medium text-blue-500 uppercase hover:text-blue-600 dark:hover:text-blue-400" href="/tags/eks">eks</a><a class="mr-3 text-sm font-medium text-blue-500 uppercase hover:text-blue-600 dark:hover:text-blue-400" href="/tags/karpenter">karpenter</a><a class="mr-3 text-sm font-medium text-blue-500 uppercase hover:text-blue-600 dark:hover:text-blue-400" href="/tags/devops">devops</a></div></div><div class="flex justify-between py-4 xl:block xl:space-y-8 xl:py-8"><div><h2 class="text-xs tracking-wide text-gray-500 uppercase dark:text-gray-400">Previous Article</h2><div class="text-blue-500 hover:text-blue-600 dark:hover:text-blue-400"><a href="/blog/ros-and-k8s">Kubernetes and ROS are surprisingly similar</a></div></div></div></div><div class="pt-4 xl:pt-8"><a class="text-blue-500 hover:text-blue-600 dark:hover:text-blue-400" href="/blog">← Back to the blog</a></div></footer></div></div></article></div></main><footer><div class="flex flex-col items-center mt-12"><div class="flex mb-3 space-x-4"><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://github.com/abhishekshree"><span class="sr-only">github</span><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" class="fill-current text-gray-700 dark:text-gray-200 hover:text-blue-500 dark:hover:text-blue-400 h-4 w-4"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg></a><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://facebook.com/abhishek.shree30"><span class="sr-only">facebook</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="fill-current text-gray-700 dark:text-gray-200 hover:text-blue-500 dark:hover:text-blue-400 h-4 w-4"><path d="M24 12.073c0-6.627-5.373-12-12-12s-12 5.373-12 12c0 5.99 4.388 10.954 10.125 11.854v-8.385H7.078v-3.47h3.047V9.43c0-3.007 1.792-4.669 4.533-4.669 1.312 0 2.686.235 2.686.235v2.953H15.83c-1.491 0-1.956.925-1.956 1.874v2.25h3.328l-.532 3.47h-2.796v8.385C19.612 23.027 24 18.062 24 12.073z"></path></svg></a><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://www.linkedin.com/in/abhishekshree/"><span class="sr-only">linkedin</span><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" class="fill-current text-gray-700 dark:text-gray-200 hover:text-blue-500 dark:hover:text-blue-400 h-4 w-4"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path></svg></a><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://twitter.com/AbhishekShree10"><span class="sr-only">twitter</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="fill-current text-gray-700 dark:text-gray-200 hover:text-blue-500 dark:hover:text-blue-400 h-4 w-4"><path d="M23.953 4.57a10 10 0 01-2.825.775 4.958 4.958 0 002.163-2.723c-.951.555-2.005.959-3.127 1.184a4.92 4.92 0 00-8.384 4.482C7.69 8.095 4.067 6.13 1.64 3.162a4.822 4.822 0 00-.666 2.475c0 1.71.87 3.213 2.188 4.096a4.904 4.904 0 01-2.228-.616v.06a4.923 4.923 0 003.946 4.827 4.996 4.996 0 01-2.212.085 4.936 4.936 0 004.604 3.417 9.867 9.867 0 01-6.102 2.105c-.39 0-.779-.023-1.17-.067a13.995 13.995 0 007.557 2.209c9.053 0 13.998-7.496 13.998-13.985 0-.21 0-.42-.015-.63A9.935 9.935 0 0024 4.59z"></path></svg></a></div><div class="flex mono mb-2 space-x-2 text-sm text-gray-500 dark:text-gray-400"><div>© 2025</div><div> • </div><a href="/">Abhishek Shree</a></div></div></footer></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"mdxSource":{"compiledSource":"\"use strict\";\n\nvar _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i \u003c arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i \u003c sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) \u003e= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i \u003c sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) \u003e= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar layoutProps = {};\nvar MDXLayout = \"wrapper\";\n\nfunction MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"Picture this: It's 2021, and I'm watching a friend at a B2B SaaS startup manually deploying updates to their customer data platform. SSH into server 1, pull the latest code, restart the service. SSH into server 2, repeat. Pray nothing breaks. Fast forward to some time later, that same company is processing millions of events daily across hundreds of microservices, all orchestrated by Kubernetes.\"), mdx(\"h1\", {\n    \"id\": \"first-encounter-with-kubernetes\"\n  }, mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#first-encounter-with-kubernetes\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"First encounter with Kubernetes\"), mdx(\"p\", null, \"Modern B2B SaaS companies like whatever, Slack, Paradime ;), etc are distributed computing platforms represented as simple web apps. Behind Slack's chat interface are hundreds of microservices handling message routing, file uploads, search indexing, and notifications. Saw a similar thing with Paradime. Each service needs independent deployment, scaling, and updates across multiple regions, customers, etc. It's pretty cool to see how you keep many tenants under a centrally managable cluster and cater them with updates, patches, and new features without downtime, together.\"), mdx(\"h1\", {\n    \"id\": \"understanding-kubernetes-orchestration\"\n  }, mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#understanding-kubernetes-orchestration\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"Understanding Kubernetes Orchestration\"), mdx(\"p\", null, \"Let's clear up the infrastructure confusion first. A \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"cluster\"), \" spans multiple physical machines (nodes). Each \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"pod\"), \" runs on a single node - you can't split pods across machines, but you can run multiple pods of the same app on different nodes.\"), mdx(\"h2\", {\n    \"id\": \"how-k8s-orchestration-actually-works\"\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#how-k8s-orchestration-actually-works\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"How K8s Orchestration Actually Works\"), mdx(\"p\", null, \"Kubernetes orchestration is essentially a distributed state machine. The \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"API server\"), \" acts as the central state store, etcd holds the persistent state, and various controllers continuously work to make reality match your desired configuration.\"), mdx(\"p\", null, \"A simple orchestration flow when you deploy an application could be,\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"// Kubernetes Orchestration Flow\\nkubectl apply \\u2192 API Server \\u2192 etcd (stores desired state)\\n                    \\u2193\\n            Controller Manager watches changes\\n                    \\u2193\\n    ReplicaSet Controller creates Pod objects\\n                    \\u2193\\n        Scheduler assigns Pods to Nodes\\n                    \\u2193\\n    Kubelet on Node pulls images and starts containers\\n                    \\u2193\\n        kube-proxy configures networking rules\\n\")), mdx(\"p\", null, \"It is so much more than just container orchestration, managing networking, persistent storage, and resource allocation. Pretty cool stuff.\"), mdx(\"p\", null, \"The \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Controller Manager\"), \" runs multiple control loops, each responsible for a specific resource type. The ReplicaSet controller ensures you have the right number of pod replicas. The Deployment controller manages ReplicaSets during rolling updates. The Service controller creates endpoints for load balancing. There are probably a thousands of other blogs you could read about each of the units, so let's just leave it there. \"), mdx(\"p\", null, \"The scheduler's algorithm, I find pretty cool, is more sophisticated than just resource availability:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"// Kubernetes Scheduler (simplified)\\nfunction schedulePod(pod) {\\n    eligibleNodes = filterNodes(pod.requirements)\\n    \\n    for each node in eligibleNodes {\\n        score = 0\\n        score += leastRequestedPriority(node)  // (capacity - requests) / capacity\\n        score += nodeAffinityPriority(node, pod)\\n        score += balancedResourceAllocation(node)\\n        score += imageLocalityPriority(node, pod) // prefer nodes with images cached\\n        score += interPodAffinityPriority(node, pod) // pod placement preferences\\n        node.totalScore = score\\n    }\\n    \\n    return node with highest totalScore\\n}\\n\")), mdx(\"p\", null, \"The networking orchestration is particularly clever. Every pod gets its own IP address within a cluster-wide subnet. The \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Container Network Interface (CNI)\"), \" plugin handles the actual networking implementation. When a pod starts, the kubelet calls the CNI plugin to:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Create a network namespace for the pod\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Assign an IP address\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Configure routing rules\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Set up network policies\")), mdx(\"h2\", {\n    \"id\": \"how-pod-ips-actually-get-handed-out\"\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#how-pod-ips-actually-get-handed-out\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"How Pod IPs Actually Get Handed Out\"), mdx(\"p\", null, \"Picture Kubernetes as a very organized librarian managing IP addresses. When you create a cluster, you're essentially saying \\\"here's my IP address book\\\" - typically something like \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"10.244.0.0/16\"), \". That's your cluster CIDR, and it's finite. 65,536 IPs to be exact.\"), mdx(\"p\", null, \"The distribution looks like this:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"Your Cluster's IP Real Estate:\\n10.244.0.0/16 (65,536 total addresses)\\n    \\u2193\\nNode 1 gets: 10.244.0.0/24    (256 addresses)\\nNode 2 gets: 10.244.1.0/24    (256 addresses)  \\nNode 3 gets: 10.244.2.0/24    (256 addresses)\\n...you get the pattern\\n\")), mdx(\"p\", null, \"Each node becomes a mini IP manager. When a pod starts up:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Kubelet taps the CNI plugin on the shoulder: \\\"Hey, need an IP for this new pod\\\"\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"CNI plugin checks its local notebook: \\\"Let's see... 10.244.1.47 is free, there you go\\\"\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Creates a network namespace, assigns the IP, updates the books\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Pod gets its own little network identity\")), mdx(\"p\", null, \"Pretty elegant, right? Until reality crashes the party.\"), mdx(\"h2\", {\n    \"id\": \"the-math-that-ruins-your-day\"\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#the-math-that-ruins-your-day\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"The Math That Ruins Your Day\"), mdx(\"p\", null, \"Here's where that \\\"infinite pods\\\" dream hits some very finite walls:\"), mdx(\"h3\", {\n    \"id\": \"ip-address-space---its-not-actually-infinite\"\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#ip-address-space---its-not-actually-infinite\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"IP Address Space - It's Not Actually Infinite\"), mdx(\"p\", null, \"That \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"/16\"), \" CIDR? You're looking at ~65K IPs total. Sounds like a lot until you realize you're carving it up into \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"/24\"), \" chunks (256 IPs per node). You can theoretically have ~256 nodes, but you're losing addresses to:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Network and broadcast addresses (because networking protocols are picky)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Service IPs (different CIDR but still affects your planning)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"System pods that you forgot about\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"That one guy who hardcoded some random IP somewhere (you know who you are)\")), mdx(\"p\", null, \"Read a bit about EKS as well, AWS has some opinions about your pod density dreams. They have a hard limit on how many pods can run per instance type based on the networking capabilities of the underlying hardware, like:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"m5.large:   29 pods max   (networking hardware says no)\\nm5.xlarge:  58 pods max   (double the CPU, not double the pods)\\nm5.2xlarge: 58 pods max   (same networking limits, just prettier)\\nc5.24xlarge: 737 pods max (now we're talking, lfggg)\\n\")), mdx(\"p\", null, \"Since we are talking networks, it'll be inhuman of me to not take a dig on \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://dryairship.github.io/post/when-i-messed-up-the-gymkhana-server/\"\n  }, \"this crisis\"), \" which could've been avoided and everyone at IITK would still have wifi access if we had such network isolation. \"), mdx(\"p\", null, \"Services provide stable endpoints for pod groups using kube-proxy, which implements load balancing through iptables rules or IPVS. When you create a service, kube-proxy updates iptables on every node to redirect traffic to healthy pod IPs.\"), mdx(\"h2\", {\n    \"id\": \"resource-orchestration-deep-dive\"\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#resource-orchestration-deep-dive\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"Resource Orchestration Deep Dive\"), mdx(\"p\", null, \"Resource sharing uses Linux \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"cgroups\"), \" for enforcement and the \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Completely Fair Scheduler (CFS)\"), \" for CPU time allocation. When you request 250m CPU, that's 250 millicores (0.25 cores). Here's how it works:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"// Resource allocation logic\\nfunction allocateResources(pod, node) {\\n    // CPU is compressible - can be throttled\\n    cpuShares = pod.cpuRequest * 1024 / 1000  // Convert millicores to CPU shares\\n    setCgroupLimit(\\\"/sys/fs/cgroup/cpu/pod123/cpu.shares\\\", cpuShares)\\n    \\n    // Memory is incompressible - OOM kill if exceeded\\n    memoryLimit = pod.memoryLimit\\n    setCgroupLimit(\\\"/sys/fs/cgroup/memory/pod123/memory.limit_in_bytes\\\", memoryLimit)\\n}\\n\")), mdx(\"p\", null, \"A 4-core node (4000m total) can run 16 containers each requesting 250m, but only if they don't all peak simultaneously. The CFS ensures fair time slicing among containers sharing the same core. Very very cool stuff.\"), mdx(\"p\", null, \"The orchestration extends to storage through \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Container Storage Interface (CSI)\"), \". When a pod needs persistent storage, the scheduler considers volume topology, the CSI driver provisions storage, and the kubelet mounts it into the container.\"), mdx(\"h1\", {\n    \"id\": \"what-are-fargates\"\n  }, mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#what-are-fargates\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"What Are Fargates?\"), mdx(\"p\", null, \"Fargate is AWS's serverless compute engine that runs containers without managing infrastructure. Think Lambda for containers with more flexibility.\"), mdx(\"p\", null, \"Traditional containers require provisioning EC2 instances, managing OS, security patches, and monitoring. Fargate abstracts this away. You define container specs (CPU, memory), AWS handles everything else.\"), mdx(\"p\", null, \"The magic happens with Firecracker microVMs. Each Fargate task runs in its own lightweight VM, providing hardware-level isolation. This is why Fargate has longer cold starts than regular containers but better security boundaries. \"), mdx(\"h1\", {\n    \"id\": \"eks---awss-kubernetes-distribution\"\n  }, mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#eks---awss-kubernetes-distribution\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"EKS - AWS's Kubernetes Distribution\"), mdx(\"p\", null, \"AWS looked at Kubernetes control plane complexity and said \\\"what if we made this not terrible?\\\" Setting up production K8s involves configuring etcd, API server, scheduler, controller manager, certificates, and high availability. It's a nightmare.\"), mdx(\"p\", null, \"EKS handles this complexity. You get a managed control plane across multiple AZs with automatic backups, patching, and monitoring. Just an HTTPS endpoint to interact with your cluster.\"), mdx(\"h2\", {\n    \"id\": \"eks-with-ec2-nodes\"\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#eks-with-ec2-nodes\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"EKS with EC2 Nodes\"), mdx(\"p\", null, \"The classic approach. You get actual EC2 instances as worker nodes running the kubelet, kube-proxy, and container runtime.\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Resource Sharing\"), \": Traditional Kubernetes resource model. Multiple pods share the same EC2 instance's CPU, memory, and network. A single m5.large (2 vCPUs, 8GB RAM) might run 10-20 small pods efficiently. The kubelet enforces resource limits using cgroups, and pods compete for resources using the CFS scheduler.\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Pros\"), \": Full control, SSH access, persistent storage, privileged containers, GPU support\\n\", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Cons\"), \": You manage OS, patches, node lifecycle, capacity planning\"), mdx(\"h2\", {\n    \"id\": \"eks-with-fargate\"\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#eks-with-fargate\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"EKS with Fargate\"), mdx(\"p\", null, \"Serverless containers where each pod runs in its own Firecracker microVM.\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Resource Sharing\"), \": No sharing! Each pod gets dedicated CPU and memory allocation. You specify exact resource requirements (0.25 vCPU, 0.5GB RAM up to 4 vCPU, 30GB RAM), and AWS provisions that exact capacity. Think of it as \\\"right-sized VMs\\\" for each workload.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"// Fargate resource allocation\\nPod A: 0.5 vCPU, 1GB RAM \\u2192 Gets dedicated Fargate task with those exact resources\\nPod B: 1 vCPU, 2GB RAM   \\u2192 Gets separate dedicated Fargate task\\n\")), mdx(\"h2\", {\n    \"id\": \"mixed-mode-resource-management\"\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#mixed-mode-resource-management\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"Mixed Mode Resource Management\"), mdx(\"p\", null, \"This is where it gets interesting. EKS can intelligently distribute workloads across both compute types based on your specifications:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"// Mixed mode scheduling\\nfunction scheduleWorkload(pod) {\\n    if (pod.hasLabel(\\\"compute-type\\\", \\\"fargate\\\")) {\\n        scheduleOnFargate(pod)  // Dedicated resources\\n    } else if (pod.requiresGPU() || pod.isPrivileged()) {\\n        scheduleOnEC2(pod)      // Shared EC2 resources\\n    } else {\\n        optimizeForCost(pod)    // Scheduler picks best option\\n    }\\n}\\n\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Resource Optimization Strategies\"), \":\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Steady-state services\"), \": Run on EC2 for cost efficiency through resource sharing\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Batch jobs\"), \": Use Fargate for predictable resource allocation and no capacity planning\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Spiky workloads\"), \": Fargate for auto-scaling without pre-provisioned capacity\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Long-running stateful services\"), \": EC2 with persistent storage\")), mdx(\"p\", null, \"The EKS scheduler considers both resource availability and cost optimization. A CPU-intensive batch job might run on Fargate for dedicated performance, while a low-resource API service runs on shared EC2 instances for cost efficiency.\"), mdx(\"h1\", {\n    \"id\": \"karpenter\"\n  }, mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#karpenter\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"Karpenter\"), mdx(\"p\", null, \"Traditional Cluster Autoscaler is dumb. It monitors unschedulable pods and scales predefined node groups. Need high-memory workloads but only have m5.large node groups? Tough luck convinving AWS team that.\"), mdx(\"p\", null, \"Karpenter implements just-in-time provisioning with intelligent instance selection:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"// Karpenter's provisioning algorithm (simplified)\\nfunction provisionNodes(unschedulablePods) {\\n    requirements = aggregateRequirements(unschedulablePods)\\n    \\n    candidates = []\\n    for each instanceType in EC2_INSTANCE_TYPES {\\n        if meetsRequirements(instanceType, requirements) {\\n            score = calculateScore(instanceType, requirements)\\n            candidates.add({type: instanceType, score: score})\\n        }\\n    }\\n    \\n    // Multi-objective optimization\\n    optimal = candidates.sortBy(cost, performance, availability).first()\\n    return launchInstance(optimal.type)\\n}\\n\")), mdx(\"p\", null, \"The scoring considers cost, performance, availability, and capacity efficiency. Karpenter also implements consolidation algorithms, continuously monitoring utilization and moving pods to pack them efficiently. \"), mdx(\"p\", null, \"For spot instances, Karpenter implements diversification strategies, spreading workloads across instance families and AZs. When spot interruption arrives, it gracefully drains nodes and provisions replacement capacity. You can check it \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/aws/karpenter-provider-aws/blob/main/designs/consolidation.md\"\n  }, \"here\"), \" if you want to read more about the consolidation strategies. I was intrigued, also this blog became massive. \"), mdx(\"hr\", null), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.eksworkshop.com/\"\n  }, \"EKS Workshop\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://kubernetes.io/docs/concepts/scheduling-eviction/\"\n  }, \"Kubernetes Scheduling Deep Dive\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=lkg_9ETHeks\"\n  }, \"AWS re:Invent Karpenter Talks\"))));\n}\n\n;\nMDXContent.isMDXComponent = true;","renderedOutput":"\u003cp\u003ePicture this: It\u0026#x27;s 2021, and I\u0026#x27;m watching a friend at a B2B SaaS startup manually deploying updates to their customer data platform. SSH into server 1, pull the latest code, restart the service. SSH into server 2, repeat. Pray nothing breaks. Fast forward to some time later, that same company is processing millions of events daily across hundreds of microservices, all orchestrated by Kubernetes.\u003c/p\u003e\u003ch1 id=\"first-encounter-with-kubernetes\"\u003e\u003ca href=\"#first-encounter-with-kubernetes\" aria-hidden=\"true\" tabindex=\"-1\"\u003e\u003cspan class=\"icon icon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFirst encounter with Kubernetes\u003c/h1\u003e\u003cp\u003eModern B2B SaaS companies like whatever, Slack, Paradime ;), etc are distributed computing platforms represented as simple web apps. Behind Slack\u0026#x27;s chat interface are hundreds of microservices handling message routing, file uploads, search indexing, and notifications. Saw a similar thing with Paradime. Each service needs independent deployment, scaling, and updates across multiple regions, customers, etc. It\u0026#x27;s pretty cool to see how you keep many tenants under a centrally managable cluster and cater them with updates, patches, and new features without downtime, together.\u003c/p\u003e\u003ch1 id=\"understanding-kubernetes-orchestration\"\u003e\u003ca href=\"#understanding-kubernetes-orchestration\" aria-hidden=\"true\" tabindex=\"-1\"\u003e\u003cspan class=\"icon icon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUnderstanding Kubernetes Orchestration\u003c/h1\u003e\u003cp\u003eLet\u0026#x27;s clear up the infrastructure confusion first. A \u003cstrong\u003ecluster\u003c/strong\u003e spans multiple physical machines (nodes). Each \u003cstrong\u003epod\u003c/strong\u003e runs on a single node - you can\u0026#x27;t split pods across machines, but you can run multiple pods of the same app on different nodes.\u003c/p\u003e\u003ch2 id=\"how-k8s-orchestration-actually-works\"\u003e\u003ca href=\"#how-k8s-orchestration-actually-works\" aria-hidden=\"true\" tabindex=\"-1\"\u003e\u003cspan class=\"icon icon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow K8s Orchestration Actually Works\u003c/h2\u003e\u003cp\u003eKubernetes orchestration is essentially a distributed state machine. The \u003cstrong\u003eAPI server\u003c/strong\u003e acts as the central state store, etcd holds the persistent state, and various controllers continuously work to make reality match your desired configuration.\u003c/p\u003e\u003cp\u003eA simple orchestration flow when you deploy an application could be,\u003c/p\u003e\u003cpre\u003e\u003ccode\u003e// Kubernetes Orchestration Flow\nkubectl apply → API Server → etcd (stores desired state)\n                    ↓\n            Controller Manager watches changes\n                    ↓\n    ReplicaSet Controller creates Pod objects\n                    ↓\n        Scheduler assigns Pods to Nodes\n                    ↓\n    Kubelet on Node pulls images and starts containers\n                    ↓\n        kube-proxy configures networking rules\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIt is so much more than just container orchestration, managing networking, persistent storage, and resource allocation. Pretty cool stuff.\u003c/p\u003e\u003cp\u003eThe \u003cstrong\u003eController Manager\u003c/strong\u003e runs multiple control loops, each responsible for a specific resource type. The ReplicaSet controller ensures you have the right number of pod replicas. The Deployment controller manages ReplicaSets during rolling updates. The Service controller creates endpoints for load balancing. There are probably a thousands of other blogs you could read about each of the units, so let\u0026#x27;s just leave it there. \u003c/p\u003e\u003cp\u003eThe scheduler\u0026#x27;s algorithm, I find pretty cool, is more sophisticated than just resource availability:\u003c/p\u003e\u003cpre\u003e\u003ccode\u003e// Kubernetes Scheduler (simplified)\nfunction schedulePod(pod) {\n    eligibleNodes = filterNodes(pod.requirements)\n    \n    for each node in eligibleNodes {\n        score = 0\n        score += leastRequestedPriority(node)  // (capacity - requests) / capacity\n        score += nodeAffinityPriority(node, pod)\n        score += balancedResourceAllocation(node)\n        score += imageLocalityPriority(node, pod) // prefer nodes with images cached\n        score += interPodAffinityPriority(node, pod) // pod placement preferences\n        node.totalScore = score\n    }\n    \n    return node with highest totalScore\n}\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThe networking orchestration is particularly clever. Every pod gets its own IP address within a cluster-wide subnet. The \u003cstrong\u003eContainer Network Interface (CNI)\u003c/strong\u003e plugin handles the actual networking implementation. When a pod starts, the kubelet calls the CNI plugin to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eCreate a network namespace for the pod\u003c/li\u003e\u003cli\u003eAssign an IP address\u003c/li\u003e\u003cli\u003eConfigure routing rules\u003c/li\u003e\u003cli\u003eSet up network policies\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"how-pod-ips-actually-get-handed-out\"\u003e\u003ca href=\"#how-pod-ips-actually-get-handed-out\" aria-hidden=\"true\" tabindex=\"-1\"\u003e\u003cspan class=\"icon icon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow Pod IPs Actually Get Handed Out\u003c/h2\u003e\u003cp\u003ePicture Kubernetes as a very organized librarian managing IP addresses. When you create a cluster, you\u0026#x27;re essentially saying \u0026quot;here\u0026#x27;s my IP address book\u0026quot; - typically something like \u003ccode\u003e10.244.0.0/16\u003c/code\u003e. That\u0026#x27;s your cluster CIDR, and it\u0026#x27;s finite. 65,536 IPs to be exact.\u003c/p\u003e\u003cp\u003eThe distribution looks like this:\u003c/p\u003e\u003cpre\u003e\u003ccode\u003eYour Cluster\u0026#x27;s IP Real Estate:\n10.244.0.0/16 (65,536 total addresses)\n    ↓\nNode 1 gets: 10.244.0.0/24    (256 addresses)\nNode 2 gets: 10.244.1.0/24    (256 addresses)  \nNode 3 gets: 10.244.2.0/24    (256 addresses)\n...you get the pattern\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eEach node becomes a mini IP manager. When a pod starts up:\u003c/p\u003e\u003col\u003e\u003cli\u003eKubelet taps the CNI plugin on the shoulder: \u0026quot;Hey, need an IP for this new pod\u0026quot;\u003c/li\u003e\u003cli\u003eCNI plugin checks its local notebook: \u0026quot;Let\u0026#x27;s see... 10.244.1.47 is free, there you go\u0026quot;\u003c/li\u003e\u003cli\u003eCreates a network namespace, assigns the IP, updates the books\u003c/li\u003e\u003cli\u003ePod gets its own little network identity\u003c/li\u003e\u003c/ol\u003e\u003cp\u003ePretty elegant, right? Until reality crashes the party.\u003c/p\u003e\u003ch2 id=\"the-math-that-ruins-your-day\"\u003e\u003ca href=\"#the-math-that-ruins-your-day\" aria-hidden=\"true\" tabindex=\"-1\"\u003e\u003cspan class=\"icon icon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThe Math That Ruins Your Day\u003c/h2\u003e\u003cp\u003eHere\u0026#x27;s where that \u0026quot;infinite pods\u0026quot; dream hits some very finite walls:\u003c/p\u003e\u003ch3 id=\"ip-address-space---its-not-actually-infinite\"\u003e\u003ca href=\"#ip-address-space---its-not-actually-infinite\" aria-hidden=\"true\" tabindex=\"-1\"\u003e\u003cspan class=\"icon icon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIP Address Space - It\u0026#x27;s Not Actually Infinite\u003c/h3\u003e\u003cp\u003eThat \u003ccode\u003e/16\u003c/code\u003e CIDR? You\u0026#x27;re looking at ~65K IPs total. Sounds like a lot until you realize you\u0026#x27;re carving it up into \u003ccode\u003e/24\u003c/code\u003e chunks (256 IPs per node). You can theoretically have ~256 nodes, but you\u0026#x27;re losing addresses to:\u003c/p\u003e\u003cul\u003e\u003cli\u003eNetwork and broadcast addresses (because networking protocols are picky)\u003c/li\u003e\u003cli\u003eService IPs (different CIDR but still affects your planning)\u003c/li\u003e\u003cli\u003eSystem pods that you forgot about\u003c/li\u003e\u003cli\u003eThat one guy who hardcoded some random IP somewhere (you know who you are)\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eRead a bit about EKS as well, AWS has some opinions about your pod density dreams. They have a hard limit on how many pods can run per instance type based on the networking capabilities of the underlying hardware, like:\u003c/p\u003e\u003cpre\u003e\u003ccode\u003em5.large:   29 pods max   (networking hardware says no)\nm5.xlarge:  58 pods max   (double the CPU, not double the pods)\nm5.2xlarge: 58 pods max   (same networking limits, just prettier)\nc5.24xlarge: 737 pods max (now we\u0026#x27;re talking, lfggg)\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eSince we are talking networks, it\u0026#x27;ll be inhuman of me to not take a dig on \u003ca target=\"_blank\" rel=\"noopener noreferrer\" style=\"text-decoration:none\" href=\"https://dryairship.github.io/post/when-i-messed-up-the-gymkhana-server/\"\u003ethis crisis\u003c/a\u003e which could\u0026#x27;ve been avoided and everyone at IITK would still have wifi access if we had such network isolation. \u003c/p\u003e\u003cp\u003eServices provide stable endpoints for pod groups using kube-proxy, which implements load balancing through iptables rules or IPVS. When you create a service, kube-proxy updates iptables on every node to redirect traffic to healthy pod IPs.\u003c/p\u003e\u003ch2 id=\"resource-orchestration-deep-dive\"\u003e\u003ca href=\"#resource-orchestration-deep-dive\" aria-hidden=\"true\" tabindex=\"-1\"\u003e\u003cspan class=\"icon icon-link\"\u003e\u003c/span\u003e\u003c/a\u003eResource Orchestration Deep Dive\u003c/h2\u003e\u003cp\u003eResource sharing uses Linux \u003cstrong\u003ecgroups\u003c/strong\u003e for enforcement and the \u003cstrong\u003eCompletely Fair Scheduler (CFS)\u003c/strong\u003e for CPU time allocation. When you request 250m CPU, that\u0026#x27;s 250 millicores (0.25 cores). Here\u0026#x27;s how it works:\u003c/p\u003e\u003cpre\u003e\u003ccode\u003e// Resource allocation logic\nfunction allocateResources(pod, node) {\n    // CPU is compressible - can be throttled\n    cpuShares = pod.cpuRequest * 1024 / 1000  // Convert millicores to CPU shares\n    setCgroupLimit(\u0026quot;/sys/fs/cgroup/cpu/pod123/cpu.shares\u0026quot;, cpuShares)\n    \n    // Memory is incompressible - OOM kill if exceeded\n    memoryLimit = pod.memoryLimit\n    setCgroupLimit(\u0026quot;/sys/fs/cgroup/memory/pod123/memory.limit_in_bytes\u0026quot;, memoryLimit)\n}\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eA 4-core node (4000m total) can run 16 containers each requesting 250m, but only if they don\u0026#x27;t all peak simultaneously. The CFS ensures fair time slicing among containers sharing the same core. Very very cool stuff.\u003c/p\u003e\u003cp\u003eThe orchestration extends to storage through \u003cstrong\u003eContainer Storage Interface (CSI)\u003c/strong\u003e. When a pod needs persistent storage, the scheduler considers volume topology, the CSI driver provisions storage, and the kubelet mounts it into the container.\u003c/p\u003e\u003ch1 id=\"what-are-fargates\"\u003e\u003ca href=\"#what-are-fargates\" aria-hidden=\"true\" tabindex=\"-1\"\u003e\u003cspan class=\"icon icon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWhat Are Fargates?\u003c/h1\u003e\u003cp\u003eFargate is AWS\u0026#x27;s serverless compute engine that runs containers without managing infrastructure. Think Lambda for containers with more flexibility.\u003c/p\u003e\u003cp\u003eTraditional containers require provisioning EC2 instances, managing OS, security patches, and monitoring. Fargate abstracts this away. You define container specs (CPU, memory), AWS handles everything else.\u003c/p\u003e\u003cp\u003eThe magic happens with Firecracker microVMs. Each Fargate task runs in its own lightweight VM, providing hardware-level isolation. This is why Fargate has longer cold starts than regular containers but better security boundaries. \u003c/p\u003e\u003ch1 id=\"eks---awss-kubernetes-distribution\"\u003e\u003ca href=\"#eks---awss-kubernetes-distribution\" aria-hidden=\"true\" tabindex=\"-1\"\u003e\u003cspan class=\"icon icon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEKS - AWS\u0026#x27;s Kubernetes Distribution\u003c/h1\u003e\u003cp\u003eAWS looked at Kubernetes control plane complexity and said \u0026quot;what if we made this not terrible?\u0026quot; Setting up production K8s involves configuring etcd, API server, scheduler, controller manager, certificates, and high availability. It\u0026#x27;s a nightmare.\u003c/p\u003e\u003cp\u003eEKS handles this complexity. You get a managed control plane across multiple AZs with automatic backups, patching, and monitoring. Just an HTTPS endpoint to interact with your cluster.\u003c/p\u003e\u003ch2 id=\"eks-with-ec2-nodes\"\u003e\u003ca href=\"#eks-with-ec2-nodes\" aria-hidden=\"true\" tabindex=\"-1\"\u003e\u003cspan class=\"icon icon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEKS with EC2 Nodes\u003c/h2\u003e\u003cp\u003eThe classic approach. You get actual EC2 instances as worker nodes running the kubelet, kube-proxy, and container runtime.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eResource Sharing\u003c/strong\u003e: Traditional Kubernetes resource model. Multiple pods share the same EC2 instance\u0026#x27;s CPU, memory, and network. A single m5.large (2 vCPUs, 8GB RAM) might run 10-20 small pods efficiently. The kubelet enforces resource limits using cgroups, and pods compete for resources using the CFS scheduler.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003ePros\u003c/strong\u003e: Full control, SSH access, persistent storage, privileged containers, GPU support\n\u003cstrong\u003eCons\u003c/strong\u003e: You manage OS, patches, node lifecycle, capacity planning\u003c/p\u003e\u003ch2 id=\"eks-with-fargate\"\u003e\u003ca href=\"#eks-with-fargate\" aria-hidden=\"true\" tabindex=\"-1\"\u003e\u003cspan class=\"icon icon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEKS with Fargate\u003c/h2\u003e\u003cp\u003eServerless containers where each pod runs in its own Firecracker microVM.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eResource Sharing\u003c/strong\u003e: No sharing! Each pod gets dedicated CPU and memory allocation. You specify exact resource requirements (0.25 vCPU, 0.5GB RAM up to 4 vCPU, 30GB RAM), and AWS provisions that exact capacity. Think of it as \u0026quot;right-sized VMs\u0026quot; for each workload.\u003c/p\u003e\u003cpre\u003e\u003ccode\u003e// Fargate resource allocation\nPod A: 0.5 vCPU, 1GB RAM → Gets dedicated Fargate task with those exact resources\nPod B: 1 vCPU, 2GB RAM   → Gets separate dedicated Fargate task\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"mixed-mode-resource-management\"\u003e\u003ca href=\"#mixed-mode-resource-management\" aria-hidden=\"true\" tabindex=\"-1\"\u003e\u003cspan class=\"icon icon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMixed Mode Resource Management\u003c/h2\u003e\u003cp\u003eThis is where it gets interesting. EKS can intelligently distribute workloads across both compute types based on your specifications:\u003c/p\u003e\u003cpre\u003e\u003ccode\u003e// Mixed mode scheduling\nfunction scheduleWorkload(pod) {\n    if (pod.hasLabel(\u0026quot;compute-type\u0026quot;, \u0026quot;fargate\u0026quot;)) {\n        scheduleOnFargate(pod)  // Dedicated resources\n    } else if (pod.requiresGPU() || pod.isPrivileged()) {\n        scheduleOnEC2(pod)      // Shared EC2 resources\n    } else {\n        optimizeForCost(pod)    // Scheduler picks best option\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003cstrong\u003eResource Optimization Strategies\u003c/strong\u003e:\u003c/p\u003e\u003cul\u003e\u003cli\u003e\u003cstrong\u003eSteady-state services\u003c/strong\u003e: Run on EC2 for cost efficiency through resource sharing\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eBatch jobs\u003c/strong\u003e: Use Fargate for predictable resource allocation and no capacity planning\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eSpiky workloads\u003c/strong\u003e: Fargate for auto-scaling without pre-provisioned capacity\u003c/li\u003e\u003cli\u003e\u003cstrong\u003eLong-running stateful services\u003c/strong\u003e: EC2 with persistent storage\u003c/li\u003e\u003c/ul\u003e\u003cp\u003eThe EKS scheduler considers both resource availability and cost optimization. A CPU-intensive batch job might run on Fargate for dedicated performance, while a low-resource API service runs on shared EC2 instances for cost efficiency.\u003c/p\u003e\u003ch1 id=\"karpenter\"\u003e\u003ca href=\"#karpenter\" aria-hidden=\"true\" tabindex=\"-1\"\u003e\u003cspan class=\"icon icon-link\"\u003e\u003c/span\u003e\u003c/a\u003eKarpenter\u003c/h1\u003e\u003cp\u003eTraditional Cluster Autoscaler is dumb. It monitors unschedulable pods and scales predefined node groups. Need high-memory workloads but only have m5.large node groups? Tough luck convinving AWS team that.\u003c/p\u003e\u003cp\u003eKarpenter implements just-in-time provisioning with intelligent instance selection:\u003c/p\u003e\u003cpre\u003e\u003ccode\u003e// Karpenter\u0026#x27;s provisioning algorithm (simplified)\nfunction provisionNodes(unschedulablePods) {\n    requirements = aggregateRequirements(unschedulablePods)\n    \n    candidates = []\n    for each instanceType in EC2_INSTANCE_TYPES {\n        if meetsRequirements(instanceType, requirements) {\n            score = calculateScore(instanceType, requirements)\n            candidates.add({type: instanceType, score: score})\n        }\n    }\n    \n    // Multi-objective optimization\n    optimal = candidates.sortBy(cost, performance, availability).first()\n    return launchInstance(optimal.type)\n}\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThe scoring considers cost, performance, availability, and capacity efficiency. Karpenter also implements consolidation algorithms, continuously monitoring utilization and moving pods to pack them efficiently. \u003c/p\u003e\u003cp\u003eFor spot instances, Karpenter implements diversification strategies, spreading workloads across instance families and AZs. When spot interruption arrives, it gracefully drains nodes and provisions replacement capacity. You can check it \u003ca target=\"_blank\" rel=\"noopener noreferrer\" style=\"text-decoration:none\" href=\"https://github.com/aws/karpenter-provider-aws/blob/main/designs/consolidation.md\"\u003ehere\u003c/a\u003e if you want to read more about the consolidation strategies. I was intrigued, also this blog became massive. \u003c/p\u003e\u003chr/\u003e\u003cul\u003e\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" style=\"text-decoration:none\" href=\"https://www.eksworkshop.com/\"\u003eEKS Workshop\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" style=\"text-decoration:none\" href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/\"\u003eKubernetes Scheduling Deep Dive\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" style=\"text-decoration:none\" href=\"https://www.youtube.com/watch?v=lkg_9ETHeks\"\u003eAWS re:Invent Karpenter Talks\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e","scope":{}},"frontMatter":{"wordCount":1734,"readingTime":{"text":"9 min read","minutes":8.665,"time":519900,"words":1733},"slug":"kubernetes-a-primer","fileName":"kubernetes-a-primer.md","title":"Kubernetes, EKS and some Karpenter(y)","date":"2025-06-29","tags":["kubernetes","aws","eks","karpenter","devops"],"draft":false,"summary":"From container chaos to cluster harmony - understanding K8s, EKS flavors, and why Karpenter even exists.","images":[]}},"prev":{"title":"Kubernetes and ROS are surprisingly similar","date":"2025-06-29","tags":["kubernetes","ROS","distributed systems"],"draft":false,"summary":"Why learning ROS made Kubernetes click for me.","images":[],"slug":"ros-and-k8s"},"next":null},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"kubernetes-a-primer"},"buildId":"I-WTZFY5ZKh476NTDoWsD","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>