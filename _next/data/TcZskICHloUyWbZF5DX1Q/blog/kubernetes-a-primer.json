{"pageProps":{"post":{"mdxSource":{"compiledSource":"\"use strict\";\n\nvar _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar layoutProps = {};\nvar MDXLayout = \"wrapper\";\n\nfunction MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"Picture this: It's 2021, and I'm watching a friend at a B2B SaaS startup manually deploying updates to their customer data platform. SSH into server 1, pull the latest code, restart the service. SSH into server 2, repeat. Pray nothing breaks. Fast forward to some time later, that same company is processing millions of events daily across hundreds of microservices, all orchestrated by Kubernetes.\"), mdx(\"h1\", {\n    \"id\": \"first-encounter-with-kubernetes\"\n  }, mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#first-encounter-with-kubernetes\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"First encounter with Kubernetes\"), mdx(\"p\", null, \"Modern B2B SaaS companies like whatever, Slack, Paradime ;), etc are distributed computing platforms represented as simple web apps. Behind Slack's chat interface are hundreds of microservices handling message routing, file uploads, search indexing, and notifications. Saw a similar thing with Paradime. Each service needs independent deployment, scaling, and updates across multiple regions, customers, etc. It's pretty cool to see how you keep many tenants under a centrally managable cluster and cater them with updates, patches, and new features without downtime, together.\"), mdx(\"h1\", {\n    \"id\": \"understanding-kubernetes-orchestration\"\n  }, mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#understanding-kubernetes-orchestration\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"Understanding Kubernetes Orchestration\"), mdx(\"p\", null, \"Let's clear up the infrastructure confusion first. A \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"cluster\"), \" spans multiple physical machines (nodes). Each \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"pod\"), \" runs on a single node - you can't split pods across machines, but you can run multiple pods of the same app on different nodes.\"), mdx(\"h2\", {\n    \"id\": \"how-k8s-orchestration-actually-works\"\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#how-k8s-orchestration-actually-works\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"How K8s Orchestration Actually Works\"), mdx(\"p\", null, \"Kubernetes orchestration is essentially a distributed state machine. The \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"API server\"), \" acts as the central state store, etcd holds the persistent state, and various controllers continuously work to make reality match your desired configuration.\"), mdx(\"p\", null, \"A simple orchestration flow when you deploy an application could be,\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"// Kubernetes Orchestration Flow\\nkubectl apply \\u2192 API Server \\u2192 etcd (stores desired state)\\n                    \\u2193\\n            Controller Manager watches changes\\n                    \\u2193\\n    ReplicaSet Controller creates Pod objects\\n                    \\u2193\\n        Scheduler assigns Pods to Nodes\\n                    \\u2193\\n    Kubelet on Node pulls images and starts containers\\n                    \\u2193\\n        kube-proxy configures networking rules\\n\")), mdx(\"p\", null, \"It is so much more than just container orchestration, managing networking, persistent storage, and resource allocation. Pretty cool stuff.\"), mdx(\"p\", null, \"The \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Controller Manager\"), \" runs multiple control loops, each responsible for a specific resource type. The ReplicaSet controller ensures you have the right number of pod replicas. The Deployment controller manages ReplicaSets during rolling updates. The Service controller creates endpoints for load balancing. There are probably a thousands of other blogs you could read about each of the units, so let's just leave it there. \"), mdx(\"p\", null, \"The scheduler's algorithm, I find pretty cool, is more sophisticated than just resource availability:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"// Kubernetes Scheduler (simplified)\\nfunction schedulePod(pod) {\\n    eligibleNodes = filterNodes(pod.requirements)\\n    \\n    for each node in eligibleNodes {\\n        score = 0\\n        score += leastRequestedPriority(node)  // (capacity - requests) / capacity\\n        score += nodeAffinityPriority(node, pod)\\n        score += balancedResourceAllocation(node)\\n        score += imageLocalityPriority(node, pod) // prefer nodes with images cached\\n        score += interPodAffinityPriority(node, pod) // pod placement preferences\\n        node.totalScore = score\\n    }\\n    \\n    return node with highest totalScore\\n}\\n\")), mdx(\"p\", null, \"The networking orchestration is particularly clever. Every pod gets its own IP address within a cluster-wide subnet. The \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Container Network Interface (CNI)\"), \" plugin handles the actual networking implementation. When a pod starts, the kubelet calls the CNI plugin to:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Create a network namespace for the pod\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Assign an IP address\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Configure routing rules\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Set up network policies\")), mdx(\"h2\", {\n    \"id\": \"how-pod-ips-actually-get-handed-out\"\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#how-pod-ips-actually-get-handed-out\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"How Pod IPs Actually Get Handed Out\"), mdx(\"p\", null, \"Picture Kubernetes as a very organized librarian managing IP addresses. When you create a cluster, you're essentially saying \\\"here's my IP address book\\\" - typically something like \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"10.244.0.0/16\"), \". That's your cluster CIDR, and it's finite. 65,536 IPs to be exact.\"), mdx(\"p\", null, \"The distribution looks like this:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"Your Cluster's IP Real Estate:\\n10.244.0.0/16 (65,536 total addresses)\\n    \\u2193\\nNode 1 gets: 10.244.0.0/24    (256 addresses)\\nNode 2 gets: 10.244.1.0/24    (256 addresses)  \\nNode 3 gets: 10.244.2.0/24    (256 addresses)\\n...you get the pattern\\n\")), mdx(\"p\", null, \"Each node becomes a mini IP manager. When a pod starts up:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Kubelet taps the CNI plugin on the shoulder: \\\"Hey, need an IP for this new pod\\\"\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"CNI plugin checks its local notebook: \\\"Let's see... 10.244.1.47 is free, there you go\\\"\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Creates a network namespace, assigns the IP, updates the books\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Pod gets its own little network identity\")), mdx(\"p\", null, \"Pretty elegant, right? Until reality crashes the party.\"), mdx(\"h2\", {\n    \"id\": \"the-math-that-ruins-your-day\"\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#the-math-that-ruins-your-day\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"The Math That Ruins Your Day\"), mdx(\"p\", null, \"Here's where that \\\"infinite pods\\\" dream hits some very finite walls:\"), mdx(\"h3\", {\n    \"id\": \"ip-address-space---its-not-actually-infinite\"\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"#ip-address-space---its-not-actually-infinite\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"IP Address Space - It's Not Actually Infinite\"), mdx(\"p\", null, \"That \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"/16\"), \" CIDR? You're looking at ~65K IPs total. Sounds like a lot until you realize you're carving it up into \", mdx(\"inlineCode\", {\n    parentName: \"p\"\n  }, \"/24\"), \" chunks (256 IPs per node). You can theoretically have ~256 nodes, but you're losing addresses to:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Network and broadcast addresses (because networking protocols are picky)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Service IPs (different CIDR but still affects your planning)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"System pods that you forgot about\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"That one guy who hardcoded some random IP somewhere (you know who you are)\")), mdx(\"p\", null, \"Read a bit about EKS as well, AWS has some opinions about your pod density dreams. They have a hard limit on how many pods can run per instance type based on the networking capabilities of the underlying hardware, like:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"m5.large:   29 pods max   (networking hardware says no)\\nm5.xlarge:  58 pods max   (double the CPU, not double the pods)\\nm5.2xlarge: 58 pods max   (same networking limits, just prettier)\\nc5.24xlarge: 737 pods max (now we're talking, lfggg)\\n\")), mdx(\"p\", null, \"Since we are talking networks, it'll be inhuman of me to not take a dig on \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://dryairship.github.io/post/when-i-messed-up-the-gymkhana-server/\"\n  }, \"this crisis\"), \" which could've been avoided and everyone at IITK would still have wifi access if we had such network isolation. \"), mdx(\"p\", null, \"Services provide stable endpoints for pod groups using kube-proxy, which implements load balancing through iptables rules or IPVS. When you create a service, kube-proxy updates iptables on every node to redirect traffic to healthy pod IPs.\"), mdx(\"h2\", {\n    \"id\": \"resource-orchestration-deep-dive\"\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#resource-orchestration-deep-dive\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"Resource Orchestration Deep Dive\"), mdx(\"p\", null, \"Resource sharing uses Linux \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"cgroups\"), \" for enforcement and the \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Completely Fair Scheduler (CFS)\"), \" for CPU time allocation. When you request 250m CPU, that's 250 millicores (0.25 cores). Here's how it works:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"// Resource allocation logic\\nfunction allocateResources(pod, node) {\\n    // CPU is compressible - can be throttled\\n    cpuShares = pod.cpuRequest * 1024 / 1000  // Convert millicores to CPU shares\\n    setCgroupLimit(\\\"/sys/fs/cgroup/cpu/pod123/cpu.shares\\\", cpuShares)\\n    \\n    // Memory is incompressible - OOM kill if exceeded\\n    memoryLimit = pod.memoryLimit\\n    setCgroupLimit(\\\"/sys/fs/cgroup/memory/pod123/memory.limit_in_bytes\\\", memoryLimit)\\n}\\n\")), mdx(\"p\", null, \"A 4-core node (4000m total) can run 16 containers each requesting 250m, but only if they don't all peak simultaneously. The CFS ensures fair time slicing among containers sharing the same core. Very very cool stuff.\"), mdx(\"p\", null, \"The orchestration extends to storage through \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Container Storage Interface (CSI)\"), \". When a pod needs persistent storage, the scheduler considers volume topology, the CSI driver provisions storage, and the kubelet mounts it into the container.\"), mdx(\"h1\", {\n    \"id\": \"what-are-fargates\"\n  }, mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#what-are-fargates\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"What Are Fargates?\"), mdx(\"p\", null, \"Fargate is AWS's serverless compute engine that runs containers without managing infrastructure. Think Lambda for containers with more flexibility.\"), mdx(\"p\", null, \"Traditional containers require provisioning EC2 instances, managing OS, security patches, and monitoring. Fargate abstracts this away. You define container specs (CPU, memory), AWS handles everything else.\"), mdx(\"p\", null, \"The magic happens with Firecracker microVMs. Each Fargate task runs in its own lightweight VM, providing hardware-level isolation. This is why Fargate has longer cold starts than regular containers but better security boundaries. \"), mdx(\"h1\", {\n    \"id\": \"eks---awss-kubernetes-distribution\"\n  }, mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#eks---awss-kubernetes-distribution\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"EKS - AWS's Kubernetes Distribution\"), mdx(\"p\", null, \"AWS looked at Kubernetes control plane complexity and said \\\"what if we made this not terrible?\\\" Setting up production K8s involves configuring etcd, API server, scheduler, controller manager, certificates, and high availability. It's a nightmare.\"), mdx(\"p\", null, \"EKS handles this complexity. You get a managed control plane across multiple AZs with automatic backups, patching, and monitoring. Just an HTTPS endpoint to interact with your cluster.\"), mdx(\"h2\", {\n    \"id\": \"eks-with-ec2-nodes\"\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#eks-with-ec2-nodes\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"EKS with EC2 Nodes\"), mdx(\"p\", null, \"The classic approach. You get actual EC2 instances as worker nodes running the kubelet, kube-proxy, and container runtime.\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Resource Sharing\"), \": Traditional Kubernetes resource model. Multiple pods share the same EC2 instance's CPU, memory, and network. A single m5.large (2 vCPUs, 8GB RAM) might run 10-20 small pods efficiently. The kubelet enforces resource limits using cgroups, and pods compete for resources using the CFS scheduler.\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Pros\"), \": Full control, SSH access, persistent storage, privileged containers, GPU support\\n\", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Cons\"), \": You manage OS, patches, node lifecycle, capacity planning\"), mdx(\"h2\", {\n    \"id\": \"eks-with-fargate\"\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#eks-with-fargate\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"EKS with Fargate\"), mdx(\"p\", null, \"Serverless containers where each pod runs in its own Firecracker microVM.\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Resource Sharing\"), \": No sharing! Each pod gets dedicated CPU and memory allocation. You specify exact resource requirements (0.25 vCPU, 0.5GB RAM up to 4 vCPU, 30GB RAM), and AWS provisions that exact capacity. Think of it as \\\"right-sized VMs\\\" for each workload.\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"// Fargate resource allocation\\nPod A: 0.5 vCPU, 1GB RAM \\u2192 Gets dedicated Fargate task with those exact resources\\nPod B: 1 vCPU, 2GB RAM   \\u2192 Gets separate dedicated Fargate task\\n\")), mdx(\"h2\", {\n    \"id\": \"mixed-mode-resource-management\"\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"href\": \"#mixed-mode-resource-management\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"Mixed Mode Resource Management\"), mdx(\"p\", null, \"This is where it gets interesting. EKS can intelligently distribute workloads across both compute types based on your specifications:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"// Mixed mode scheduling\\nfunction scheduleWorkload(pod) {\\n    if (pod.hasLabel(\\\"compute-type\\\", \\\"fargate\\\")) {\\n        scheduleOnFargate(pod)  // Dedicated resources\\n    } else if (pod.requiresGPU() || pod.isPrivileged()) {\\n        scheduleOnEC2(pod)      // Shared EC2 resources\\n    } else {\\n        optimizeForCost(pod)    // Scheduler picks best option\\n    }\\n}\\n\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Resource Optimization Strategies\"), \":\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Steady-state services\"), \": Run on EC2 for cost efficiency through resource sharing\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Batch jobs\"), \": Use Fargate for predictable resource allocation and no capacity planning\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Spiky workloads\"), \": Fargate for auto-scaling without pre-provisioned capacity\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Long-running stateful services\"), \": EC2 with persistent storage\")), mdx(\"p\", null, \"The EKS scheduler considers both resource availability and cost optimization. A CPU-intensive batch job might run on Fargate for dedicated performance, while a low-resource API service runs on shared EC2 instances for cost efficiency.\"), mdx(\"h1\", {\n    \"id\": \"karpenter\"\n  }, mdx(\"a\", {\n    parentName: \"h1\",\n    \"href\": \"#karpenter\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"Karpenter\"), mdx(\"p\", null, \"Traditional Cluster Autoscaler is dumb. It monitors unschedulable pods and scales predefined node groups. Need high-memory workloads but only have m5.large node groups? Tough luck convinving AWS team that.\"), mdx(\"p\", null, \"Karpenter implements just-in-time provisioning with intelligent instance selection:\"), mdx(\"pre\", null, mdx(\"code\", {\n    parentName: \"pre\"\n  }, \"// Karpenter's provisioning algorithm (simplified)\\nfunction provisionNodes(unschedulablePods) {\\n    requirements = aggregateRequirements(unschedulablePods)\\n    \\n    candidates = []\\n    for each instanceType in EC2_INSTANCE_TYPES {\\n        if meetsRequirements(instanceType, requirements) {\\n            score = calculateScore(instanceType, requirements)\\n            candidates.add({type: instanceType, score: score})\\n        }\\n    }\\n    \\n    // Multi-objective optimization\\n    optimal = candidates.sortBy(cost, performance, availability).first()\\n    return launchInstance(optimal.type)\\n}\\n\")), mdx(\"p\", null, \"The scoring considers cost, performance, availability, and capacity efficiency. Karpenter also implements consolidation algorithms, continuously monitoring utilization and moving pods to pack them efficiently. \"), mdx(\"p\", null, \"For spot instances, Karpenter implements diversification strategies, spreading workloads across instance families and AZs. When spot interruption arrives, it gracefully drains nodes and provisions replacement capacity. You can check it \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/aws/karpenter-provider-aws/blob/main/designs/consolidation.md\"\n  }, \"here\"), \" if you want to read more about the consolidation strategies. I was intrigued, also this blog became massive. \"), mdx(\"hr\", null), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.eksworkshop.com/\"\n  }, \"EKS Workshop\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://kubernetes.io/docs/concepts/scheduling-eviction/\"\n  }, \"Kubernetes Scheduling Deep Dive\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=lkg_9ETHeks\"\n  }, \"AWS re:Invent Karpenter Talks\"))));\n}\n\n;\nMDXContent.isMDXComponent = true;","renderedOutput":"<p>Picture this: It&#x27;s 2021, and I&#x27;m watching a friend at a B2B SaaS startup manually deploying updates to their customer data platform. SSH into server 1, pull the latest code, restart the service. SSH into server 2, repeat. Pray nothing breaks. Fast forward to some time later, that same company is processing millions of events daily across hundreds of microservices, all orchestrated by Kubernetes.</p><h1 id=\"first-encounter-with-kubernetes\"><a href=\"#first-encounter-with-kubernetes\" aria-hidden=\"true\" tabindex=\"-1\"><span class=\"icon icon-link\"></span></a>First encounter with Kubernetes</h1><p>Modern B2B SaaS companies like whatever, Slack, Paradime ;), etc are distributed computing platforms represented as simple web apps. Behind Slack&#x27;s chat interface are hundreds of microservices handling message routing, file uploads, search indexing, and notifications. Saw a similar thing with Paradime. Each service needs independent deployment, scaling, and updates across multiple regions, customers, etc. It&#x27;s pretty cool to see how you keep many tenants under a centrally managable cluster and cater them with updates, patches, and new features without downtime, together.</p><h1 id=\"understanding-kubernetes-orchestration\"><a href=\"#understanding-kubernetes-orchestration\" aria-hidden=\"true\" tabindex=\"-1\"><span class=\"icon icon-link\"></span></a>Understanding Kubernetes Orchestration</h1><p>Let&#x27;s clear up the infrastructure confusion first. A <strong>cluster</strong> spans multiple physical machines (nodes). Each <strong>pod</strong> runs on a single node - you can&#x27;t split pods across machines, but you can run multiple pods of the same app on different nodes.</p><h2 id=\"how-k8s-orchestration-actually-works\"><a href=\"#how-k8s-orchestration-actually-works\" aria-hidden=\"true\" tabindex=\"-1\"><span class=\"icon icon-link\"></span></a>How K8s Orchestration Actually Works</h2><p>Kubernetes orchestration is essentially a distributed state machine. The <strong>API server</strong> acts as the central state store, etcd holds the persistent state, and various controllers continuously work to make reality match your desired configuration.</p><p>A simple orchestration flow when you deploy an application could be,</p><pre><code>// Kubernetes Orchestration Flow\nkubectl apply → API Server → etcd (stores desired state)\n                    ↓\n            Controller Manager watches changes\n                    ↓\n    ReplicaSet Controller creates Pod objects\n                    ↓\n        Scheduler assigns Pods to Nodes\n                    ↓\n    Kubelet on Node pulls images and starts containers\n                    ↓\n        kube-proxy configures networking rules\n</code></pre><p>It is so much more than just container orchestration, managing networking, persistent storage, and resource allocation. Pretty cool stuff.</p><p>The <strong>Controller Manager</strong> runs multiple control loops, each responsible for a specific resource type. The ReplicaSet controller ensures you have the right number of pod replicas. The Deployment controller manages ReplicaSets during rolling updates. The Service controller creates endpoints for load balancing. There are probably a thousands of other blogs you could read about each of the units, so let&#x27;s just leave it there. </p><p>The scheduler&#x27;s algorithm, I find pretty cool, is more sophisticated than just resource availability:</p><pre><code>// Kubernetes Scheduler (simplified)\nfunction schedulePod(pod) {\n    eligibleNodes = filterNodes(pod.requirements)\n    \n    for each node in eligibleNodes {\n        score = 0\n        score += leastRequestedPriority(node)  // (capacity - requests) / capacity\n        score += nodeAffinityPriority(node, pod)\n        score += balancedResourceAllocation(node)\n        score += imageLocalityPriority(node, pod) // prefer nodes with images cached\n        score += interPodAffinityPriority(node, pod) // pod placement preferences\n        node.totalScore = score\n    }\n    \n    return node with highest totalScore\n}\n</code></pre><p>The networking orchestration is particularly clever. Every pod gets its own IP address within a cluster-wide subnet. The <strong>Container Network Interface (CNI)</strong> plugin handles the actual networking implementation. When a pod starts, the kubelet calls the CNI plugin to:</p><ul><li>Create a network namespace for the pod</li><li>Assign an IP address</li><li>Configure routing rules</li><li>Set up network policies</li></ul><h2 id=\"how-pod-ips-actually-get-handed-out\"><a href=\"#how-pod-ips-actually-get-handed-out\" aria-hidden=\"true\" tabindex=\"-1\"><span class=\"icon icon-link\"></span></a>How Pod IPs Actually Get Handed Out</h2><p>Picture Kubernetes as a very organized librarian managing IP addresses. When you create a cluster, you&#x27;re essentially saying &quot;here&#x27;s my IP address book&quot; - typically something like <code>10.244.0.0/16</code>. That&#x27;s your cluster CIDR, and it&#x27;s finite. 65,536 IPs to be exact.</p><p>The distribution looks like this:</p><pre><code>Your Cluster&#x27;s IP Real Estate:\n10.244.0.0/16 (65,536 total addresses)\n    ↓\nNode 1 gets: 10.244.0.0/24    (256 addresses)\nNode 2 gets: 10.244.1.0/24    (256 addresses)  \nNode 3 gets: 10.244.2.0/24    (256 addresses)\n...you get the pattern\n</code></pre><p>Each node becomes a mini IP manager. When a pod starts up:</p><ol><li>Kubelet taps the CNI plugin on the shoulder: &quot;Hey, need an IP for this new pod&quot;</li><li>CNI plugin checks its local notebook: &quot;Let&#x27;s see... 10.244.1.47 is free, there you go&quot;</li><li>Creates a network namespace, assigns the IP, updates the books</li><li>Pod gets its own little network identity</li></ol><p>Pretty elegant, right? Until reality crashes the party.</p><h2 id=\"the-math-that-ruins-your-day\"><a href=\"#the-math-that-ruins-your-day\" aria-hidden=\"true\" tabindex=\"-1\"><span class=\"icon icon-link\"></span></a>The Math That Ruins Your Day</h2><p>Here&#x27;s where that &quot;infinite pods&quot; dream hits some very finite walls:</p><h3 id=\"ip-address-space---its-not-actually-infinite\"><a href=\"#ip-address-space---its-not-actually-infinite\" aria-hidden=\"true\" tabindex=\"-1\"><span class=\"icon icon-link\"></span></a>IP Address Space - It&#x27;s Not Actually Infinite</h3><p>That <code>/16</code> CIDR? You&#x27;re looking at ~65K IPs total. Sounds like a lot until you realize you&#x27;re carving it up into <code>/24</code> chunks (256 IPs per node). You can theoretically have ~256 nodes, but you&#x27;re losing addresses to:</p><ul><li>Network and broadcast addresses (because networking protocols are picky)</li><li>Service IPs (different CIDR but still affects your planning)</li><li>System pods that you forgot about</li><li>That one guy who hardcoded some random IP somewhere (you know who you are)</li></ul><p>Read a bit about EKS as well, AWS has some opinions about your pod density dreams. They have a hard limit on how many pods can run per instance type based on the networking capabilities of the underlying hardware, like:</p><pre><code>m5.large:   29 pods max   (networking hardware says no)\nm5.xlarge:  58 pods max   (double the CPU, not double the pods)\nm5.2xlarge: 58 pods max   (same networking limits, just prettier)\nc5.24xlarge: 737 pods max (now we&#x27;re talking, lfggg)\n</code></pre><p>Since we are talking networks, it&#x27;ll be inhuman of me to not take a dig on <a target=\"_blank\" rel=\"noopener noreferrer\" style=\"text-decoration:none\" href=\"https://dryairship.github.io/post/when-i-messed-up-the-gymkhana-server/\">this crisis</a> which could&#x27;ve been avoided and everyone at IITK would still have wifi access if we had such network isolation. </p><p>Services provide stable endpoints for pod groups using kube-proxy, which implements load balancing through iptables rules or IPVS. When you create a service, kube-proxy updates iptables on every node to redirect traffic to healthy pod IPs.</p><h2 id=\"resource-orchestration-deep-dive\"><a href=\"#resource-orchestration-deep-dive\" aria-hidden=\"true\" tabindex=\"-1\"><span class=\"icon icon-link\"></span></a>Resource Orchestration Deep Dive</h2><p>Resource sharing uses Linux <strong>cgroups</strong> for enforcement and the <strong>Completely Fair Scheduler (CFS)</strong> for CPU time allocation. When you request 250m CPU, that&#x27;s 250 millicores (0.25 cores). Here&#x27;s how it works:</p><pre><code>// Resource allocation logic\nfunction allocateResources(pod, node) {\n    // CPU is compressible - can be throttled\n    cpuShares = pod.cpuRequest * 1024 / 1000  // Convert millicores to CPU shares\n    setCgroupLimit(&quot;/sys/fs/cgroup/cpu/pod123/cpu.shares&quot;, cpuShares)\n    \n    // Memory is incompressible - OOM kill if exceeded\n    memoryLimit = pod.memoryLimit\n    setCgroupLimit(&quot;/sys/fs/cgroup/memory/pod123/memory.limit_in_bytes&quot;, memoryLimit)\n}\n</code></pre><p>A 4-core node (4000m total) can run 16 containers each requesting 250m, but only if they don&#x27;t all peak simultaneously. The CFS ensures fair time slicing among containers sharing the same core. Very very cool stuff.</p><p>The orchestration extends to storage through <strong>Container Storage Interface (CSI)</strong>. When a pod needs persistent storage, the scheduler considers volume topology, the CSI driver provisions storage, and the kubelet mounts it into the container.</p><h1 id=\"what-are-fargates\"><a href=\"#what-are-fargates\" aria-hidden=\"true\" tabindex=\"-1\"><span class=\"icon icon-link\"></span></a>What Are Fargates?</h1><p>Fargate is AWS&#x27;s serverless compute engine that runs containers without managing infrastructure. Think Lambda for containers with more flexibility.</p><p>Traditional containers require provisioning EC2 instances, managing OS, security patches, and monitoring. Fargate abstracts this away. You define container specs (CPU, memory), AWS handles everything else.</p><p>The magic happens with Firecracker microVMs. Each Fargate task runs in its own lightweight VM, providing hardware-level isolation. This is why Fargate has longer cold starts than regular containers but better security boundaries. </p><h1 id=\"eks---awss-kubernetes-distribution\"><a href=\"#eks---awss-kubernetes-distribution\" aria-hidden=\"true\" tabindex=\"-1\"><span class=\"icon icon-link\"></span></a>EKS - AWS&#x27;s Kubernetes Distribution</h1><p>AWS looked at Kubernetes control plane complexity and said &quot;what if we made this not terrible?&quot; Setting up production K8s involves configuring etcd, API server, scheduler, controller manager, certificates, and high availability. It&#x27;s a nightmare.</p><p>EKS handles this complexity. You get a managed control plane across multiple AZs with automatic backups, patching, and monitoring. Just an HTTPS endpoint to interact with your cluster.</p><h2 id=\"eks-with-ec2-nodes\"><a href=\"#eks-with-ec2-nodes\" aria-hidden=\"true\" tabindex=\"-1\"><span class=\"icon icon-link\"></span></a>EKS with EC2 Nodes</h2><p>The classic approach. You get actual EC2 instances as worker nodes running the kubelet, kube-proxy, and container runtime.</p><p><strong>Resource Sharing</strong>: Traditional Kubernetes resource model. Multiple pods share the same EC2 instance&#x27;s CPU, memory, and network. A single m5.large (2 vCPUs, 8GB RAM) might run 10-20 small pods efficiently. The kubelet enforces resource limits using cgroups, and pods compete for resources using the CFS scheduler.</p><p><strong>Pros</strong>: Full control, SSH access, persistent storage, privileged containers, GPU support\n<strong>Cons</strong>: You manage OS, patches, node lifecycle, capacity planning</p><h2 id=\"eks-with-fargate\"><a href=\"#eks-with-fargate\" aria-hidden=\"true\" tabindex=\"-1\"><span class=\"icon icon-link\"></span></a>EKS with Fargate</h2><p>Serverless containers where each pod runs in its own Firecracker microVM.</p><p><strong>Resource Sharing</strong>: No sharing! Each pod gets dedicated CPU and memory allocation. You specify exact resource requirements (0.25 vCPU, 0.5GB RAM up to 4 vCPU, 30GB RAM), and AWS provisions that exact capacity. Think of it as &quot;right-sized VMs&quot; for each workload.</p><pre><code>// Fargate resource allocation\nPod A: 0.5 vCPU, 1GB RAM → Gets dedicated Fargate task with those exact resources\nPod B: 1 vCPU, 2GB RAM   → Gets separate dedicated Fargate task\n</code></pre><h2 id=\"mixed-mode-resource-management\"><a href=\"#mixed-mode-resource-management\" aria-hidden=\"true\" tabindex=\"-1\"><span class=\"icon icon-link\"></span></a>Mixed Mode Resource Management</h2><p>This is where it gets interesting. EKS can intelligently distribute workloads across both compute types based on your specifications:</p><pre><code>// Mixed mode scheduling\nfunction scheduleWorkload(pod) {\n    if (pod.hasLabel(&quot;compute-type&quot;, &quot;fargate&quot;)) {\n        scheduleOnFargate(pod)  // Dedicated resources\n    } else if (pod.requiresGPU() || pod.isPrivileged()) {\n        scheduleOnEC2(pod)      // Shared EC2 resources\n    } else {\n        optimizeForCost(pod)    // Scheduler picks best option\n    }\n}\n</code></pre><p><strong>Resource Optimization Strategies</strong>:</p><ul><li><strong>Steady-state services</strong>: Run on EC2 for cost efficiency through resource sharing</li><li><strong>Batch jobs</strong>: Use Fargate for predictable resource allocation and no capacity planning</li><li><strong>Spiky workloads</strong>: Fargate for auto-scaling without pre-provisioned capacity</li><li><strong>Long-running stateful services</strong>: EC2 with persistent storage</li></ul><p>The EKS scheduler considers both resource availability and cost optimization. A CPU-intensive batch job might run on Fargate for dedicated performance, while a low-resource API service runs on shared EC2 instances for cost efficiency.</p><h1 id=\"karpenter\"><a href=\"#karpenter\" aria-hidden=\"true\" tabindex=\"-1\"><span class=\"icon icon-link\"></span></a>Karpenter</h1><p>Traditional Cluster Autoscaler is dumb. It monitors unschedulable pods and scales predefined node groups. Need high-memory workloads but only have m5.large node groups? Tough luck convinving AWS team that.</p><p>Karpenter implements just-in-time provisioning with intelligent instance selection:</p><pre><code>// Karpenter&#x27;s provisioning algorithm (simplified)\nfunction provisionNodes(unschedulablePods) {\n    requirements = aggregateRequirements(unschedulablePods)\n    \n    candidates = []\n    for each instanceType in EC2_INSTANCE_TYPES {\n        if meetsRequirements(instanceType, requirements) {\n            score = calculateScore(instanceType, requirements)\n            candidates.add({type: instanceType, score: score})\n        }\n    }\n    \n    // Multi-objective optimization\n    optimal = candidates.sortBy(cost, performance, availability).first()\n    return launchInstance(optimal.type)\n}\n</code></pre><p>The scoring considers cost, performance, availability, and capacity efficiency. Karpenter also implements consolidation algorithms, continuously monitoring utilization and moving pods to pack them efficiently. </p><p>For spot instances, Karpenter implements diversification strategies, spreading workloads across instance families and AZs. When spot interruption arrives, it gracefully drains nodes and provisions replacement capacity. You can check it <a target=\"_blank\" rel=\"noopener noreferrer\" style=\"text-decoration:none\" href=\"https://github.com/aws/karpenter-provider-aws/blob/main/designs/consolidation.md\">here</a> if you want to read more about the consolidation strategies. I was intrigued, also this blog became massive. </p><hr/><ul><li><a target=\"_blank\" rel=\"noopener noreferrer\" style=\"text-decoration:none\" href=\"https://www.eksworkshop.com/\">EKS Workshop</a></li><li><a target=\"_blank\" rel=\"noopener noreferrer\" style=\"text-decoration:none\" href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/\">Kubernetes Scheduling Deep Dive</a></li><li><a target=\"_blank\" rel=\"noopener noreferrer\" style=\"text-decoration:none\" href=\"https://www.youtube.com/watch?v=lkg_9ETHeks\">AWS re:Invent Karpenter Talks</a></li></ul>","scope":{}},"frontMatter":{"wordCount":1734,"readingTime":{"text":"9 min read","minutes":8.665,"time":519900,"words":1733},"slug":"kubernetes-a-primer","fileName":"kubernetes-a-primer.md","title":"Kubernetes, EKS and some Karpenter(y)","date":"2025-06-29","tags":["kubernetes","aws","eks","karpenter","devops"],"draft":false,"summary":"From container chaos to cluster harmony - understanding K8s, EKS flavors, and why Karpenter even exists.","images":[]}},"prev":{"title":"Kubernetes and ROS are surprisingly similar","date":"2025-06-29","tags":["kubernetes","ROS","distributed systems"],"draft":false,"summary":"Why learning ROS made Kubernetes click for me.","images":[],"slug":"ros-and-k8s"},"next":null},"__N_SSG":true}